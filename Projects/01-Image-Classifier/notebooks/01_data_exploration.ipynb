{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Data Exploration for Image Classification\n",
    "\n",
    "## Learning Objectives\n",
    "- Load and explore the CIFAR-10 dataset\n",
    "- Understand image data structure and properties\n",
    "- Visualize sample images and class distributions\n",
    "- Analyze dataset statistics and characteristics\n",
    "- Identify potential challenges and preprocessing needs\n",
    "\n",
    "## Dataset Overview: CIFAR-10\n",
    "\n",
    "The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes:\n",
    "- **Classes**: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\n",
    "- **Training set**: 50,000 images\n",
    "- **Test set**: 10,000 images\n",
    "- **Image size**: 32√ó32√ó3 (RGB)\n",
    "- **Balanced**: 6,000 images per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Define class names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Training set shape: {x_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test set shape: {x_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")\n",
    "print(f\"Class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate basic statistics\n",
    "total_images = len(x_train) + len(x_test)\n",
    "image_height, image_width, channels = x_train.shape[1:]\n",
    "\n",
    "print(\"üìä Dataset Statistics\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Total images: {total_images:,}\")\n",
    "print(f\"Training images: {len(x_train):,}\")\n",
    "print(f\"Test images: {len(x_test):,}\")\n",
    "print(f\"Image dimensions: {image_height}√ó{image_width}√ó{channels}\")\n",
    "print(f\"Data type: {x_train.dtype}\")\n",
    "print(f\"Pixel value range: [{x_train.min()}, {x_train.max()}]\")\n",
    "print(f\"Memory usage (training): {x_train.nbytes / (1024**2):.1f} MB\")\n",
    "print(f\"Memory usage (test): {x_test.nbytes / (1024**2):.1f} MB\")\n",
    "\n",
    "# Label statistics\n",
    "print(f\"\\nüè∑Ô∏è Label Statistics\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Label data type: {y_train.dtype}\")\n",
    "print(f\"Label range: [{y_train.min()}, {y_train.max()}]\")\n",
    "print(f\"Unique labels: {np.unique(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution\n",
    "train_counts = Counter(y_train.flatten())\n",
    "test_counts = Counter(y_test.flatten())\n",
    "\n",
    "# Create distribution dataframe\n",
    "distribution_data = []\n",
    "for i, class_name in enumerate(class_names):\n",
    "    distribution_data.append({\n",
    "        'Class': class_name,\n",
    "        'Class_ID': i,\n",
    "        'Train_Count': train_counts[i],\n",
    "        'Test_Count': test_counts[i],\n",
    "        'Total_Count': train_counts[i] + test_counts[i],\n",
    "        'Train_Percentage': (train_counts[i] / len(y_train)) * 100,\n",
    "        'Test_Percentage': (test_counts[i] / len(y_test)) * 100\n",
    "    })\n",
    "\n",
    "df_distribution = pd.DataFrame(distribution_data)\n",
    "print(\"üìà Class Distribution\")\n",
    "print(df_distribution.to_string(index=False))\n",
    "\n",
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Training set distribution\n",
    "axes[0].bar(range(len(class_names)), [train_counts[i] for i in range(len(class_names))], \n",
    "           color='skyblue', alpha=0.8, edgecolor='black')\n",
    "axes[0].set_xlabel('Classes')\n",
    "axes[0].set_ylabel('Number of Images')\n",
    "axes[0].set_title('Training Set Class Distribution')\n",
    "axes[0].set_xticks(range(len(class_names)))\n",
    "axes[0].set_xticklabels(class_names, rotation=45, ha='right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add count labels on bars\n",
    "for i, count in enumerate([train_counts[i] for i in range(len(class_names))]):\n",
    "    axes[0].text(i, count + 50, str(count), ha='center', va='bottom')\n",
    "\n",
    "# Test set distribution\n",
    "axes[1].bar(range(len(class_names)), [test_counts[i] for i in range(len(class_names))], \n",
    "           color='lightcoral', alpha=0.8, edgecolor='black')\n",
    "axes[1].set_xlabel('Classes')\n",
    "axes[1].set_ylabel('Number of Images')\n",
    "axes[1].set_title('Test Set Class Distribution')\n",
    "axes[1].set_xticks(range(len(class_names)))\n",
    "axes[1].set_xticklabels(class_names, rotation=45, ha='right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add count labels on bars\n",
    "for i, count in enumerate([test_counts[i] for i in range(len(class_names))]):\n",
    "    axes[1].text(i, count + 20, str(count), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check if dataset is balanced\n",
    "train_std = np.std([train_counts[i] for i in range(len(class_names))])\n",
    "test_std = np.std([test_counts[i] for i in range(len(class_names))])\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è Balance Analysis\")\n",
    "print(f\"Training set standard deviation: {train_std:.2f}\")\n",
    "print(f\"Test set standard deviation: {test_std:.2f}\")\n",
    "print(f\"Dataset is {'balanced' if train_std == 0 and test_std == 0 else 'imbalanced'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sample Image Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images from each class\n",
    "def plot_sample_images(x_data, y_data, class_names, samples_per_class=5):\n",
    "    \"\"\"\n",
    "    Plot sample images from each class\n",
    "    \"\"\"\n",
    "    num_classes = len(class_names)\n",
    "    fig, axes = plt.subplots(num_classes, samples_per_class, \n",
    "                            figsize=(samples_per_class * 2, num_classes * 2))\n",
    "    \n",
    "    for class_idx in range(num_classes):\n",
    "        # Find indices for current class\n",
    "        class_indices = np.where(y_data.flatten() == class_idx)[0]\n",
    "        \n",
    "        # Randomly sample images from this class\n",
    "        sample_indices = np.random.choice(class_indices, samples_per_class, replace=False)\n",
    "        \n",
    "        for sample_idx in range(samples_per_class):\n",
    "            img_idx = sample_indices[sample_idx]\n",
    "            image = x_data[img_idx]\n",
    "            \n",
    "            axes[class_idx, sample_idx].imshow(image)\n",
    "            axes[class_idx, sample_idx].axis('off')\n",
    "            \n",
    "            # Add class name to first image of each row\n",
    "            if sample_idx == 0:\n",
    "                axes[class_idx, sample_idx].set_ylabel(class_names[class_idx], \n",
    "                                                      rotation=0, ha='right', va='center')\n",
    "    \n",
    "    plt.suptitle('Sample Images from Each Class', fontsize=16, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot sample images\n",
    "plot_sample_images(x_train, y_train, class_names, samples_per_class=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pixel Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze pixel value distributions\n",
    "def analyze_pixel_statistics(x_data, title_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Analyze and visualize pixel value statistics\n",
    "    \"\"\"\n",
    "    # Calculate statistics for each channel\n",
    "    stats = {}\n",
    "    channel_names = ['Red', 'Green', 'Blue']\n",
    "    \n",
    "    for i, channel in enumerate(channel_names):\n",
    "        channel_data = x_data[:, :, :, i].flatten()\n",
    "        stats[channel] = {\n",
    "            'mean': np.mean(channel_data),\n",
    "            'std': np.std(channel_data),\n",
    "            'min': np.min(channel_data),\n",
    "            'max': np.max(channel_data),\n",
    "            'median': np.median(channel_data)\n",
    "        }\n",
    "    \n",
    "    # Overall statistics\n",
    "    overall_data = x_data.flatten()\n",
    "    stats['Overall'] = {\n",
    "        'mean': np.mean(overall_data),\n",
    "        'std': np.std(overall_data),\n",
    "        'min': np.min(overall_data),\n",
    "        'max': np.max(overall_data),\n",
    "        'median': np.median(overall_data)\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Calculate statistics for training and test sets\n",
    "train_stats = analyze_pixel_statistics(x_train, \"Training\")\n",
    "test_stats = analyze_pixel_statistics(x_test, \"Test\")\n",
    "\n",
    "# Display statistics\n",
    "print(\"üé® Pixel Value Statistics\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Channel':<10} {'Mean':<8} {'Std':<8} {'Min':<5} {'Max':<5} {'Median':<8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for channel in ['Red', 'Green', 'Blue', 'Overall']:\n",
    "    stats = train_stats[channel]\n",
    "    print(f\"{channel:<10} {stats['mean']:<8.1f} {stats['std']:<8.1f} \"\n",
    "          f\"{stats['min']:<5.0f} {stats['max']:<5.0f} {stats['median']:<8.1f}\")\n",
    "\n",
    "# Visualize pixel value distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Overall pixel distribution\n",
    "axes[0, 0].hist(x_train.flatten(), bins=50, alpha=0.7, color='skyblue', \n",
    "               edgecolor='black', density=True)\n",
    "axes[0, 0].set_title('Overall Pixel Value Distribution (Training)')\n",
    "axes[0, 0].set_xlabel('Pixel Value')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Per-channel distributions\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, (color, channel) in enumerate(zip(colors, ['Red', 'Green', 'Blue'])):\n",
    "    channel_data = x_train[:, :, :, i].flatten()\n",
    "    axes[0, 1].hist(channel_data, bins=50, alpha=0.5, color=color, \n",
    "                   label=channel, density=True)\n",
    "\n",
    "axes[0, 1].set_title('Per-Channel Pixel Distributions')\n",
    "axes[0, 1].set_xlabel('Pixel Value')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Mean pixel values per class\n",
    "class_means = []\n",
    "for class_idx in range(len(class_names)):\n",
    "    class_mask = (y_train.flatten() == class_idx)\n",
    "    class_images = x_train[class_mask]\n",
    "    class_mean = np.mean(class_images)\n",
    "    class_means.append(class_mean)\n",
    "\n",
    "axes[1, 0].bar(range(len(class_names)), class_means, color='lightgreen', \n",
    "              alpha=0.8, edgecolor='black')\n",
    "axes[1, 0].set_title('Mean Pixel Value per Class')\n",
    "axes[1, 0].set_xlabel('Class')\n",
    "axes[1, 0].set_ylabel('Mean Pixel Value')\n",
    "axes[1, 0].set_xticks(range(len(class_names)))\n",
    "axes[1, 0].set_xticklabels(class_names, rotation=45, ha='right')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Standard deviation per class\n",
    "class_stds = []\n",
    "for class_idx in range(len(class_names)):\n",
    "    class_mask = (y_train.flatten() == class_idx)\n",
    "    class_images = x_train[class_mask]\n",
    "    class_std = np.std(class_images)\n",
    "    class_stds.append(class_std)\n",
    "\n",
    "axes[1, 1].bar(range(len(class_names)), class_stds, color='orange', \n",
    "              alpha=0.8, edgecolor='black')\n",
    "axes[1, 1].set_title('Pixel Value Std Dev per Class')\n",
    "axes[1, 1].set_xlabel('Class')\n",
    "axes[1, 1].set_ylabel('Standard Deviation')\n",
    "axes[1, 1].set_xticks(range(len(class_names)))\n",
    "axes[1, 1].set_xticklabels(class_names, rotation=45, ha='right')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Image Complexity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze image complexity using various metrics\n",
    "def calculate_image_complexity(images, sample_size=1000):\n",
    "    \"\"\"\n",
    "    Calculate various complexity metrics for images\n",
    "    \"\"\"\n",
    "    # Sample random images for analysis\n",
    "    indices = np.random.choice(len(images), min(sample_size, len(images)), replace=False)\n",
    "    sample_images = images[indices]\n",
    "    \n",
    "    complexities = []\n",
    "    \n",
    "    for img in sample_images:\n",
    "        # Convert to grayscale for some metrics\n",
    "        gray = np.dot(img[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "        \n",
    "        # Calculate various complexity metrics\n",
    "        metrics = {\n",
    "            'variance': np.var(img),\n",
    "            'gradient_magnitude': np.mean(np.abs(np.gradient(gray))),\n",
    "            'edge_density': np.sum(np.abs(np.gradient(gray))) / (32 * 32),\n",
    "            'color_diversity': len(np.unique(img.reshape(-1, 3), axis=0)),\n",
    "            'brightness': np.mean(img)\n",
    "        }\n",
    "        complexities.append(metrics)\n",
    "    \n",
    "    return complexities\n",
    "\n",
    "# Calculate complexity for sample images\n",
    "print(\"üîç Analyzing image complexity...\")\n",
    "complexities = calculate_image_complexity(x_train, sample_size=1000)\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "df_complexity = pd.DataFrame(complexities)\n",
    "\n",
    "print(\"\\nüìä Image Complexity Statistics\")\n",
    "print(df_complexity.describe())\n",
    "\n",
    "# Visualize complexity distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics = ['variance', 'gradient_magnitude', 'edge_density', 'color_diversity', 'brightness']\n",
    "titles = ['Pixel Variance', 'Gradient Magnitude', 'Edge Density', 'Color Diversity', 'Brightness']\n",
    "\n",
    "for i, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    axes[i].hist(df_complexity[metric], bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
    "    axes[i].set_title(f'{title} Distribution')\n",
    "    axes[i].set_xlabel(title)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Correlation heatmap\n",
    "correlation_matrix = df_complexity.corr()\n",
    "im = axes[5].imshow(correlation_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "axes[5].set_title('Complexity Metrics Correlation')\n",
    "axes[5].set_xticks(range(len(metrics)))\n",
    "axes[5].set_yticks(range(len(metrics)))\n",
    "axes[5].set_xticklabels(titles, rotation=45, ha='right')\n",
    "axes[5].set_yticklabels(titles)\n",
    "\n",
    "# Add correlation values to heatmap\n",
    "for i in range(len(metrics)):\n",
    "    for j in range(len(metrics)):\n",
    "        text = axes[5].text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}',\n",
    "                           ha=\"center\", va=\"center\", color=\"black\" if abs(correlation_matrix.iloc[i, j]) < 0.5 else \"white\")\n",
    "\n",
    "plt.colorbar(im, ax=axes[5], shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Class-wise Visual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average image for each class\n",
    "def calculate_class_averages(x_data, y_data, class_names):\n",
    "    \"\"\"\n",
    "    Calculate average image for each class\n",
    "    \"\"\"\n",
    "    class_averages = []\n",
    "    \n",
    "    for class_idx in range(len(class_names)):\n",
    "        class_mask = (y_data.flatten() == class_idx)\n",
    "        class_images = x_data[class_mask]\n",
    "        class_average = np.mean(class_images, axis=0)\n",
    "        class_averages.append(class_average)\n",
    "    \n",
    "    return np.array(class_averages)\n",
    "\n",
    "# Calculate class averages\n",
    "class_averages = calculate_class_averages(x_train, y_train, class_names)\n",
    "\n",
    "# Visualize class averages\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (avg_img, class_name) in enumerate(zip(class_averages, class_names)):\n",
    "    # Normalize for display\n",
    "    display_img = (avg_img - avg_img.min()) / (avg_img.max() - avg_img.min())\n",
    "    \n",
    "    axes[i].imshow(display_img)\n",
    "    axes[i].set_title(f'{class_name}\\n(Average)', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Average Images per Class', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze class separability using average images\n",
    "def calculate_class_distances(class_averages):\n",
    "    \"\"\"\n",
    "    Calculate pairwise distances between class averages\n",
    "    \"\"\"\n",
    "    n_classes = len(class_averages)\n",
    "    distances = np.zeros((n_classes, n_classes))\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        for j in range(n_classes):\n",
    "            # Calculate Euclidean distance\n",
    "            distances[i, j] = np.linalg.norm(class_averages[i] - class_averages[j])\n",
    "    \n",
    "    return distances\n",
    "\n",
    "# Calculate distances\n",
    "class_distances = calculate_class_distances(class_averages)\n",
    "\n",
    "# Visualize class distance matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "im = plt.imshow(class_distances, cmap='viridis')\n",
    "plt.colorbar(im, label='Euclidean Distance')\n",
    "plt.title('Pairwise Distances Between Class Averages')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Class')\n",
    "plt.xticks(range(len(class_names)), class_names, rotation=45, ha='right')\n",
    "plt.yticks(range(len(class_names)), class_names)\n",
    "\n",
    "# Add distance values to heatmap\n",
    "for i in range(len(class_names)):\n",
    "    for j in range(len(class_names)):\n",
    "        text = plt.text(j, i, f'{class_distances[i, j]:.0f}',\n",
    "                       ha=\"center\", va=\"center\", \n",
    "                       color=\"white\" if class_distances[i, j] > np.mean(class_distances) else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find most and least similar class pairs\n",
    "# Set diagonal to infinity to ignore self-comparisons\n",
    "distance_copy = class_distances.copy()\n",
    "np.fill_diagonal(distance_copy, np.inf)\n",
    "\n",
    "# Find most similar (smallest distance)\n",
    "min_idx = np.unravel_index(np.argmin(distance_copy), distance_copy.shape)\n",
    "most_similar = (class_names[min_idx[0]], class_names[min_idx[1]], distance_copy[min_idx])\n",
    "\n",
    "# Find least similar (largest distance)\n",
    "max_idx = np.unravel_index(np.argmax(distance_copy), distance_copy.shape)\n",
    "least_similar = (class_names[max_idx[0]], class_names[max_idx[1]], distance_copy[max_idx])\n",
    "\n",
    "print(f\"\\nüîç Class Similarity Analysis\")\n",
    "print(f\"Most similar classes: {most_similar[0]} ‚Üî {most_similar[1]} (distance: {most_similar[2]:.1f})\")\n",
    "print(f\"Least similar classes: {least_similar[0]} ‚Üî {least_similar[1]} (distance: {least_similar[2]:.1f})\")\n",
    "print(f\"Average inter-class distance: {np.mean(distance_copy[distance_copy != np.inf]):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for potential data quality issues\n",
    "def assess_data_quality(x_data, y_data):\n",
    "    \"\"\"\n",
    "    Assess various data quality metrics\n",
    "    \"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # Check for missing values\n",
    "    if np.isnan(x_data).any():\n",
    "        nan_count = np.isnan(x_data).sum()\n",
    "        issues.append(f\"Found {nan_count} NaN values in images\")\n",
    "    \n",
    "    # Check for infinite values\n",
    "    if np.isinf(x_data).any():\n",
    "        inf_count = np.isinf(x_data).sum()\n",
    "        issues.append(f\"Found {inf_count} infinite values in images\")\n",
    "    \n",
    "    # Check for completely black images\n",
    "    black_images = np.sum(x_data, axis=(1, 2, 3)) == 0\n",
    "    if black_images.any():\n",
    "        black_count = black_images.sum()\n",
    "        issues.append(f\"Found {black_count} completely black images\")\n",
    "    \n",
    "    # Check for completely white images\n",
    "    white_images = np.all(x_data == 255, axis=(1, 2, 3))\n",
    "    if white_images.any():\n",
    "        white_count = white_images.sum()\n",
    "        issues.append(f\"Found {white_count} completely white images\")\n",
    "    \n",
    "    # Check for duplicate images\n",
    "    unique_images = np.unique(x_data.reshape(len(x_data), -1), axis=0)\n",
    "    if len(unique_images) < len(x_data):\n",
    "        duplicate_count = len(x_data) - len(unique_images)\n",
    "        issues.append(f\"Found {duplicate_count} duplicate images\")\n",
    "    \n",
    "    # Check label consistency\n",
    "    if y_data.min() < 0 or y_data.max() >= len(class_names):\n",
    "        issues.append(f\"Labels out of expected range [0, {len(class_names)-1}]\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "# Assess data quality\n",
    "print(\"üîç Data Quality Assessment\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "train_issues = assess_data_quality(x_train, y_train)\n",
    "test_issues = assess_data_quality(x_test, y_test)\n",
    "\n",
    "if not train_issues and not test_issues:\n",
    "    print(\"‚úÖ No data quality issues detected!\")\n",
    "else:\n",
    "    if train_issues:\n",
    "        print(\"‚ö†Ô∏è Training set issues:\")\n",
    "        for issue in train_issues:\n",
    "            print(f\"  - {issue}\")\n",
    "    \n",
    "    if test_issues:\n",
    "        print(\"‚ö†Ô∏è Test set issues:\")\n",
    "        for issue in test_issues:\n",
    "            print(f\"  - {issue}\")\n",
    "\n",
    "# Additional quality metrics\n",
    "print(f\"\\nüìä Additional Quality Metrics\")\n",
    "print(f\"Training set memory usage: {x_train.nbytes / (1024**2):.1f} MB\")\n",
    "print(f\"Test set memory usage: {x_test.nbytes / (1024**2):.1f} MB\")\n",
    "print(f\"Images per class (train): {len(x_train) // len(class_names)}\")\n",
    "print(f\"Images per class (test): {len(x_test) // len(class_names)}\")\n",
    "print(f\"Train/test split ratio: {len(x_train) / len(x_test):.1f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Preprocessing Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate preprocessing recommendations based on analysis\n",
    "def generate_preprocessing_recommendations(x_data, stats):\n",
    "    \"\"\"\n",
    "    Generate preprocessing recommendations based on data analysis\n",
    "    \"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    # Normalization recommendation\n",
    "    pixel_range = x_data.max() - x_data.min()\n",
    "    if pixel_range > 1:\n",
    "        recommendations.append(\n",
    "            \"üîß Normalize pixel values to [0, 1] range by dividing by 255\"\n",
    "        )\n",
    "    \n",
    "    # Standardization recommendation\n",
    "    overall_mean = stats['Overall']['mean']\n",
    "    overall_std = stats['Overall']['std']\n",
    "    recommendations.append(\n",
    "        f\"üìä Consider standardization: mean={overall_mean:.1f}, std={overall_std:.1f}\"\n",
    "    )\n",
    "    \n",
    "    # Data augmentation recommendation\n",
    "    recommendations.append(\n",
    "        \"üîÑ Apply data augmentation: rotation, flipping, zoom, brightness adjustment\"\n",
    "    )\n",
    "    \n",
    "    # Resize recommendation for transfer learning\n",
    "    current_size = x_data.shape[1:3]\n",
    "    if current_size != (224, 224):\n",
    "        recommendations.append(\n",
    "            f\"üìè Resize images to 224√ó224 for transfer learning (current: {current_size[0]}√ó{current_size[1]})\"\n",
    "        )\n",
    "    \n",
    "    # Memory optimization\n",
    "    if x_data.dtype != np.float32:\n",
    "        recommendations.append(\n",
    "            \"üíæ Convert to float32 for memory efficiency and GPU compatibility\"\n",
    "        )\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Generate recommendations\n",
    "recommendations = generate_preprocessing_recommendations(x_train, train_stats)\n",
    "\n",
    "print(\"üí° Preprocessing Recommendations\")\n",
    "print(\"=\" * 50)\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "# Show example of normalization effect\n",
    "sample_image = x_train[0]\n",
    "normalized_image = sample_image.astype(np.float32) / 255.0\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axes[0].imshow(sample_image)\n",
    "axes[0].set_title(f'Original\\nRange: [{sample_image.min()}, {sample_image.max()}]')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(normalized_image)\n",
    "axes[1].set_title(f'Normalized\\nRange: [{normalized_image.min():.2f}, {normalized_image.max():.2f}]')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.suptitle('Normalization Example')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Summary and Next Steps\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Dataset Structure**: \n",
    "   - 50,000 training images, 10,000 test images\n",
    "   - 10 balanced classes with 5,000 training images each\n",
    "   - 32√ó32√ó3 RGB images\n",
    "\n",
    "2. **Data Quality**: \n",
    "   - No missing or invalid values detected\n",
    "   - Balanced class distribution\n",
    "   - Pixel values in [0, 255] range\n",
    "\n",
    "3. **Complexity Analysis**:\n",
    "   - Varying levels of image complexity across classes\n",
    "   - Some classes are more visually similar than others\n",
    "   - Good diversity in color and texture patterns\n",
    "\n",
    "### Preprocessing Pipeline:\n",
    "\n",
    "Based on our analysis, the recommended preprocessing steps are:\n",
    "\n",
    "1. **Normalization**: Scale pixel values to [0, 1]\n",
    "2. **Data Augmentation**: Apply transformations to increase dataset diversity\n",
    "3. **Resizing**: Resize to 224√ó224 for transfer learning models\n",
    "4. **Data Type**: Convert to float32 for efficiency\n",
    "\n",
    "### Expected Challenges:\n",
    "\n",
    "1. **Low Resolution**: 32√ó32 images have limited detail\n",
    "2. **Similar Classes**: Some classes (e.g., cat/dog) may be harder to distinguish\n",
    "3. **Intra-class Variation**: High variation within classes\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Implement Preprocessing Pipeline** ‚Üí `02_preprocessing.ipynb`\n",
    "2. **Build CNN Architecture** ‚Üí `03_model_building.ipynb`\n",
    "3. **Train and Evaluate Models** ‚Üí `04_training.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to move to preprocessing?** The next notebook will implement the preprocessing pipeline based on our findings here! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}