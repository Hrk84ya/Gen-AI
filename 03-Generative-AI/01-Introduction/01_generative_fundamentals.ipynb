{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Generative Models\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand probability distributions and their role in generative modeling\n",
    "- Learn about maximum likelihood estimation (MLE)\n",
    "- Explore latent variable models\n",
    "- Implement basic generative models from scratch\n",
    "- Understand sampling techniques\n",
    "\n",
    "## What Makes a Model \"Generative\"?\n",
    "\n",
    "A generative model learns the joint probability distribution P(X, Y) or the data distribution P(X), allowing it to:\n",
    "1. **Generate new samples** that look like the training data\n",
    "2. **Estimate probabilities** of new data points\n",
    "3. **Handle missing data** through conditional distributions\n",
    "4. **Provide uncertainty estimates** for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Probability Distributions: The Foundation\n",
    "\n",
    "### Understanding Data Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data from different distributions\n",
    "n_samples = 1000\n",
    "\n",
    "# 1. Normal distribution\n",
    "normal_data = np.random.normal(0, 1, n_samples)\n",
    "\n",
    "# 2. Bimodal distribution (mixture of two normals)\n",
    "bimodal_data = np.concatenate([\n",
    "    np.random.normal(-2, 0.5, n_samples//2),\n",
    "    np.random.normal(2, 0.5, n_samples//2)\n",
    "])\n",
    "\n",
    "# 3. Exponential distribution\n",
    "exp_data = np.random.exponential(1, n_samples)\n",
    "\n",
    "# 4. Uniform distribution\n",
    "uniform_data = np.random.uniform(-3, 3, n_samples)\n",
    "\n",
    "# Visualize different distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "distributions = [\n",
    "    (normal_data, 'Normal Distribution', axes[0, 0]),\n",
    "    (bimodal_data, 'Bimodal Distribution', axes[0, 1]),\n",
    "    (exp_data, 'Exponential Distribution', axes[1, 0]),\n",
    "    (uniform_data, 'Uniform Distribution', axes[1, 1])\n",
    "]\n",
    "\n",
    "for data, title, ax in distributions:\n",
    "    ax.hist(data, bins=50, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Statistics:\")\n",
    "for data, name in zip([normal_data, bimodal_data, exp_data, uniform_data], \n",
    "                     ['Normal', 'Bimodal', 'Exponential', 'Uniform']):\n",
    "    print(f\"{name:12} - Mean: {np.mean(data):.2f}, Std: {np.std(data):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "MLE finds the parameters Î¸ that maximize the likelihood of observing the data:\n",
    "\n",
    "$$\\theta^* = \\arg\\max_\\theta \\prod_{i=1}^{n} P(x_i | \\theta) = \\arg\\max_\\theta \\sum_{i=1}^{n} \\log P(x_i | \\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_likelihood(data, mu, sigma):\n",
    "    \"\"\"Calculate likelihood of data under Gaussian distribution\"\"\"\n",
    "    n = len(data)\n",
    "    log_likelihood = -n/2 * np.log(2 * np.pi * sigma**2) - np.sum((data - mu)**2) / (2 * sigma**2)\n",
    "    return log_likelihood\n",
    "\n",
    "def mle_gaussian(data):\n",
    "    \"\"\"Maximum Likelihood Estimation for Gaussian distribution\"\"\"\n",
    "    mu_mle = np.mean(data)\n",
    "    sigma_mle = np.std(data, ddof=0)  # MLE uses N, not N-1\n",
    "    return mu_mle, sigma_mle\n",
    "\n",
    "# Generate sample data\n",
    "true_mu, true_sigma = 2.5, 1.2\n",
    "sample_data = np.random.normal(true_mu, true_sigma, 500)\n",
    "\n",
    "# Estimate parameters using MLE\n",
    "estimated_mu, estimated_sigma = mle_gaussian(sample_data)\n",
    "\n",
    "print(f\"True parameters:      Î¼ = {true_mu:.2f}, Ïƒ = {true_sigma:.2f}\")\n",
    "print(f\"MLE estimates:        Î¼ = {estimated_mu:.2f}, Ïƒ = {estimated_sigma:.2f}\")\n",
    "print(f\"Sample mean/std:      Î¼ = {np.mean(sample_data):.2f}, Ïƒ = {np.std(sample_data):.2f}\")\n",
    "\n",
    "# Visualize likelihood surface\n",
    "mu_range = np.linspace(1.5, 3.5, 50)\n",
    "sigma_range = np.linspace(0.8, 1.8, 50)\n",
    "MU, SIGMA = np.meshgrid(mu_range, sigma_range)\n",
    "\n",
    "# Calculate likelihood for each parameter combination\n",
    "likelihood_surface = np.zeros_like(MU)\n",
    "for i in range(len(mu_range)):\n",
    "    for j in range(len(sigma_range)):\n",
    "        likelihood_surface[j, i] = gaussian_likelihood(sample_data, MU[j, i], SIGMA[j, i])\n",
    "\n",
    "# Plot likelihood surface\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# 3D surface plot\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "surf = ax1.plot_surface(MU, SIGMA, likelihood_surface, cmap='viridis', alpha=0.8)\n",
    "ax1.scatter([estimated_mu], [estimated_sigma], [gaussian_likelihood(sample_data, estimated_mu, estimated_sigma)], \n",
    "           color='red', s=100, label='MLE Estimate')\n",
    "ax1.set_xlabel('Î¼')\n",
    "ax1.set_ylabel('Ïƒ')\n",
    "ax1.set_zlabel('Log-Likelihood')\n",
    "ax1.set_title('Likelihood Surface')\n",
    "\n",
    "# Contour plot\n",
    "ax2 = fig.add_subplot(132)\n",
    "contour = ax2.contour(MU, SIGMA, likelihood_surface, levels=20)\n",
    "ax2.scatter(estimated_mu, estimated_sigma, color='red', s=100, marker='x', linewidth=3, label='MLE Estimate')\n",
    "ax2.scatter(true_mu, true_sigma, color='green', s=100, marker='o', label='True Parameters')\n",
    "ax2.set_xlabel('Î¼')\n",
    "ax2.set_ylabel('Ïƒ')\n",
    "ax2.set_title('Likelihood Contours')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Data histogram with fitted distribution\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.hist(sample_data, bins=30, density=True, alpha=0.7, color='lightblue', edgecolor='black', label='Data')\n",
    "x_range = np.linspace(sample_data.min(), sample_data.max(), 100)\n",
    "true_pdf = stats.norm.pdf(x_range, true_mu, true_sigma)\n",
    "estimated_pdf = stats.norm.pdf(x_range, estimated_mu, estimated_sigma)\n",
    "ax3.plot(x_range, true_pdf, 'g-', linewidth=2, label=f'True: N({true_mu:.1f}, {true_sigma:.1f})')\n",
    "ax3.plot(x_range, estimated_pdf, 'r--', linewidth=2, label=f'MLE: N({estimated_mu:.1f}, {estimated_sigma:.1f})')\n",
    "ax3.set_xlabel('Value')\n",
    "ax3.set_ylabel('Density')\n",
    "ax3.set_title('Fitted Distribution')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gaussian Mixture Models (GMM)\n",
    "\n",
    "GMMs model data as a mixture of multiple Gaussian distributions:\n",
    "\n",
    "$$P(x) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGMM:\n",
    "    def __init__(self, n_components=2, max_iter=100, tol=1e-6):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        \n",
    "    def fit(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.weights = np.ones(self.n_components) / self.n_components\n",
    "        self.means = X[np.random.choice(n_samples, self.n_components, replace=False)]\n",
    "        self.covariances = [np.eye(n_features) for _ in range(self.n_components)]\n",
    "        \n",
    "        log_likelihood_old = -np.inf\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            # E-step: Calculate responsibilities\n",
    "            responsibilities = self._e_step(X)\n",
    "            \n",
    "            # M-step: Update parameters\n",
    "            self._m_step(X, responsibilities)\n",
    "            \n",
    "            # Check convergence\n",
    "            log_likelihood = self._log_likelihood(X)\n",
    "            if abs(log_likelihood - log_likelihood_old) < self.tol:\n",
    "                print(f\"Converged after {iteration + 1} iterations\")\n",
    "                break\n",
    "            log_likelihood_old = log_likelihood\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _e_step(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        responsibilities = np.zeros((n_samples, self.n_components))\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            responsibilities[:, k] = self.weights[k] * stats.multivariate_normal.pdf(\n",
    "                X, self.means[k], self.covariances[k]\n",
    "            )\n",
    "        \n",
    "        # Normalize responsibilities\n",
    "        responsibilities /= responsibilities.sum(axis=1, keepdims=True)\n",
    "        return responsibilities\n",
    "    \n",
    "    def _m_step(self, X, responsibilities):\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            # Update weights\n",
    "            Nk = responsibilities[:, k].sum()\n",
    "            self.weights[k] = Nk / n_samples\n",
    "            \n",
    "            # Update means\n",
    "            self.means[k] = (responsibilities[:, k][:, np.newaxis] * X).sum(axis=0) / Nk\n",
    "            \n",
    "            # Update covariances\n",
    "            diff = X - self.means[k]\n",
    "            self.covariances[k] = np.dot(\n",
    "                (responsibilities[:, k][:, np.newaxis] * diff).T, diff\n",
    "            ) / Nk\n",
    "    \n",
    "    def _log_likelihood(self, X):\n",
    "        likelihood = np.zeros(X.shape[0])\n",
    "        for k in range(self.n_components):\n",
    "            likelihood += self.weights[k] * stats.multivariate_normal.pdf(\n",
    "                X, self.means[k], self.covariances[k]\n",
    "            )\n",
    "        return np.sum(np.log(likelihood))\n",
    "    \n",
    "    def sample(self, n_samples):\n",
    "        \"\"\"Generate samples from the fitted GMM\"\"\"\n",
    "        # Choose components based on weights\n",
    "        components = np.random.choice(self.n_components, n_samples, p=self.weights)\n",
    "        \n",
    "        samples = []\n",
    "        for i in range(n_samples):\n",
    "            k = components[i]\n",
    "            sample = np.random.multivariate_normal(self.means[k], self.covariances[k])\n",
    "            samples.append(sample)\n",
    "        \n",
    "        return np.array(samples)\n",
    "\n",
    "# Generate sample data (mixture of 3 Gaussians)\n",
    "np.random.seed(42)\n",
    "X, y_true = make_blobs(n_samples=300, centers=3, n_features=2, \n",
    "                       random_state=42, cluster_std=1.5)\n",
    "\n",
    "# Fit our GMM\n",
    "gmm_custom = SimpleGMM(n_components=3)\n",
    "gmm_custom.fit(X)\n",
    "\n",
    "# Fit sklearn GMM for comparison\n",
    "gmm_sklearn = GaussianMixture(n_components=3, random_state=42)\n",
    "gmm_sklearn.fit(X)\n",
    "\n",
    "# Generate samples from both models\n",
    "samples_custom = gmm_custom.sample(300)\n",
    "samples_sklearn, _ = gmm_sklearn.sample(300)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Original data\n",
    "axes[0, 0].scatter(X[:, 0], X[:, 1], c=y_true, alpha=0.7)\n",
    "axes[0, 0].set_title('Original Data')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Custom GMM samples\n",
    "axes[0, 1].scatter(samples_custom[:, 0], samples_custom[:, 1], alpha=0.7, color='red')\n",
    "axes[0, 1].set_title('Custom GMM Samples')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Sklearn GMM samples\n",
    "axes[1, 0].scatter(samples_sklearn[:, 0], samples_sklearn[:, 1], alpha=0.7, color='green')\n",
    "axes[1, 0].set_title('Sklearn GMM Samples')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Comparison\n",
    "axes[1, 1].scatter(X[:, 0], X[:, 1], alpha=0.5, label='Original', color='blue')\n",
    "axes[1, 1].scatter(samples_custom[:, 0], samples_custom[:, 1], alpha=0.5, label='Custom GMM', color='red')\n",
    "axes[1, 1].scatter(samples_sklearn[:, 0], samples_sklearn[:, 1], alpha=0.5, label='Sklearn GMM', color='green')\n",
    "axes[1, 1].set_title('Comparison')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nModel Parameters:\")\n",
    "print(f\"Custom GMM weights: {gmm_custom.weights}\")\n",
    "print(f\"Sklearn GMM weights: {gmm_sklearn.weights_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Latent Variable Models\n",
    "\n",
    "Latent variable models assume that observed data is generated from hidden (latent) variables:\n",
    "\n",
    "$$P(x) = \\int P(x|z) P(z) dz$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLatentVariableModel:\n",
    "    def __init__(self, latent_dim=2, observed_dim=4):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.observed_dim = observed_dim\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.W = np.random.randn(observed_dim, latent_dim) * 0.5  # Loading matrix\n",
    "        self.mu = np.zeros(observed_dim)  # Mean\n",
    "        self.sigma2 = 1.0  # Noise variance\n",
    "    \n",
    "    def generate_data(self, n_samples):\n",
    "        \"\"\"Generate data from the latent variable model\"\"\"\n",
    "        # Sample latent variables\n",
    "        Z = np.random.randn(n_samples, self.latent_dim)\n",
    "        \n",
    "        # Generate observations\n",
    "        X = Z @ self.W.T + self.mu + np.random.randn(n_samples, self.observed_dim) * np.sqrt(self.sigma2)\n",
    "        \n",
    "        return X, Z\n",
    "    \n",
    "    def fit(self, X, max_iter=100):\n",
    "        \"\"\"Fit the model using EM algorithm (simplified)\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        for iteration in range(max_iter):\n",
    "            # E-step: Compute posterior of latent variables\n",
    "            # For simplicity, we'll use a closed-form solution for linear Gaussian models\n",
    "            \n",
    "            # M-step: Update parameters\n",
    "            # This is a simplified version - full EM would be more complex\n",
    "            \n",
    "            # Update mean\n",
    "            self.mu = np.mean(X, axis=0)\n",
    "            \n",
    "            # Center the data\n",
    "            X_centered = X - self.mu\n",
    "            \n",
    "            # Update W using PCA-like approach\n",
    "            U, s, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
    "            self.W = Vt[:self.latent_dim].T * np.sqrt(s[:self.latent_dim] / n_samples)\n",
    "            \n",
    "            # Update noise variance\n",
    "            reconstruction = (X_centered @ self.W) @ self.W.T\n",
    "            self.sigma2 = np.mean((X_centered - reconstruction) ** 2)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform data to latent space\"\"\"\n",
    "        X_centered = X - self.mu\n",
    "        # Simplified posterior mean\n",
    "        M = self.W.T @ self.W + self.sigma2 * np.eye(self.latent_dim)\n",
    "        Z_mean = X_centered @ self.W @ np.linalg.inv(M)\n",
    "        return Z_mean\n",
    "    \n",
    "    def reconstruct(self, Z):\n",
    "        \"\"\"Reconstruct data from latent variables\"\"\"\n",
    "        return Z @ self.W.T + self.mu\n",
    "\n",
    "# Generate synthetic data\n",
    "true_model = SimpleLatentVariableModel(latent_dim=2, observed_dim=4)\n",
    "X_true, Z_true = true_model.generate_data(500)\n",
    "\n",
    "# Fit model to the data\n",
    "learned_model = SimpleLatentVariableModel(latent_dim=2, observed_dim=4)\n",
    "learned_model.fit(X_true)\n",
    "\n",
    "# Transform data to latent space\n",
    "Z_inferred = learned_model.transform(X_true)\n",
    "\n",
    "# Reconstruct data\n",
    "X_reconstructed = learned_model.reconstruct(Z_inferred)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# True latent variables\n",
    "axes[0, 0].scatter(Z_true[:, 0], Z_true[:, 1], alpha=0.7, color='blue')\n",
    "axes[0, 0].set_title('True Latent Variables')\n",
    "axes[0, 0].set_xlabel('Z1')\n",
    "axes[0, 0].set_ylabel('Z2')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Inferred latent variables\n",
    "axes[0, 1].scatter(Z_inferred[:, 0], Z_inferred[:, 1], alpha=0.7, color='red')\n",
    "axes[0, 1].set_title('Inferred Latent Variables')\n",
    "axes[0, 1].set_xlabel('Z1')\n",
    "axes[0, 1].set_ylabel('Z2')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Comparison of latent variables\n",
    "axes[0, 2].scatter(Z_true[:, 0], Z_inferred[:, 0], alpha=0.7, label='Dimension 1')\n",
    "axes[0, 2].scatter(Z_true[:, 1], Z_inferred[:, 1], alpha=0.7, label='Dimension 2')\n",
    "axes[0, 2].plot([-3, 3], [-3, 3], 'k--', alpha=0.5)\n",
    "axes[0, 2].set_title('True vs Inferred Latent Variables')\n",
    "axes[0, 2].set_xlabel('True Z')\n",
    "axes[0, 2].set_ylabel('Inferred Z')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Original data (first two dimensions)\n",
    "axes[1, 0].scatter(X_true[:, 0], X_true[:, 1], alpha=0.7, color='blue')\n",
    "axes[1, 0].set_title('Original Data (Dim 1 vs 2)')\n",
    "axes[1, 0].set_xlabel('X1')\n",
    "axes[1, 0].set_ylabel('X2')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Reconstructed data (first two dimensions)\n",
    "axes[1, 1].scatter(X_reconstructed[:, 0], X_reconstructed[:, 1], alpha=0.7, color='red')\n",
    "axes[1, 1].set_title('Reconstructed Data (Dim 1 vs 2)')\n",
    "axes[1, 1].set_xlabel('X1')\n",
    "axes[1, 1].set_ylabel('X2')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Reconstruction error\n",
    "reconstruction_error = np.mean((X_true - X_reconstructed) ** 2, axis=1)\n",
    "axes[1, 2].hist(reconstruction_error, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[1, 2].set_title('Reconstruction Error Distribution')\n",
    "axes[1, 2].set_xlabel('Mean Squared Error')\n",
    "axes[1, 2].set_ylabel('Frequency')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean reconstruction error: {np.mean(reconstruction_error):.4f}\")\n",
    "print(f\"Learned noise variance: {learned_model.sigma2:.4f}\")\n",
    "print(f\"True noise variance: {true_model.sigma2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sampling Techniques\n",
    "\n",
    "### Different methods to generate samples from distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Inverse Transform Sampling\n",
    "def inverse_transform_exponential(n_samples, lam=1.0):\n",
    "    \"\"\"Generate exponential samples using inverse transform\"\"\"\n",
    "    u = np.random.uniform(0, 1, n_samples)\n",
    "    return -np.log(1 - u) / lam\n",
    "\n",
    "# 2. Rejection Sampling\n",
    "def rejection_sampling_beta(n_samples, alpha=2, beta=3):\n",
    "    \"\"\"Generate Beta distribution samples using rejection sampling\"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    # Find maximum of the target distribution for normalization\n",
    "    x_max = (alpha - 1) / (alpha + beta - 2) if alpha > 1 and beta > 1 else 0.5\n",
    "    f_max = stats.beta.pdf(x_max, alpha, beta)\n",
    "    \n",
    "    while len(samples) < n_samples:\n",
    "        # Propose from uniform distribution\n",
    "        x = np.random.uniform(0, 1)\n",
    "        y = np.random.uniform(0, f_max)\n",
    "        \n",
    "        # Accept or reject\n",
    "        if y <= stats.beta.pdf(x, alpha, beta):\n",
    "            samples.append(x)\n",
    "    \n",
    "    return np.array(samples)\n",
    "\n",
    "# 3. Metropolis-Hastings Sampling\n",
    "def metropolis_hastings_normal(n_samples, target_mean=0, target_std=1, proposal_std=0.5):\n",
    "    \"\"\"Sample from normal distribution using Metropolis-Hastings\"\"\"\n",
    "    samples = []\n",
    "    current = 0.0  # Starting point\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Propose new state\n",
    "        proposal = current + np.random.normal(0, proposal_std)\n",
    "        \n",
    "        # Calculate acceptance probability\n",
    "        current_prob = stats.norm.pdf(current, target_mean, target_std)\n",
    "        proposal_prob = stats.norm.pdf(proposal, target_mean, target_std)\n",
    "        \n",
    "        alpha = min(1, proposal_prob / current_prob)\n",
    "        \n",
    "        # Accept or reject\n",
    "        if np.random.uniform() < alpha:\n",
    "            current = proposal\n",
    "        \n",
    "        samples.append(current)\n",
    "    \n",
    "    return np.array(samples)\n",
    "\n",
    "# Generate samples using different methods\n",
    "n_samples = 1000\n",
    "\n",
    "# Exponential distribution\n",
    "exp_inverse = inverse_transform_exponential(n_samples, lam=1.5)\n",
    "exp_direct = np.random.exponential(1/1.5, n_samples)\n",
    "\n",
    "# Beta distribution\n",
    "beta_rejection = rejection_sampling_beta(n_samples, alpha=2, beta=3)\n",
    "beta_direct = np.random.beta(2, 3, n_samples)\n",
    "\n",
    "# Normal distribution\n",
    "normal_mcmc = metropolis_hastings_normal(n_samples, target_mean=2, target_std=1.5)\n",
    "normal_direct = np.random.normal(2, 1.5, n_samples)\n",
    "\n",
    "# Visualize sampling results\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 15))\n",
    "\n",
    "# Exponential distribution\n",
    "axes[0, 0].hist(exp_inverse, bins=30, alpha=0.7, density=True, label='Inverse Transform', color='blue')\n",
    "axes[0, 0].hist(exp_direct, bins=30, alpha=0.7, density=True, label='Direct Sampling', color='red')\n",
    "x_exp = np.linspace(0, 5, 100)\n",
    "axes[0, 0].plot(x_exp, stats.expon.pdf(x_exp, scale=1/1.5), 'k-', linewidth=2, label='True PDF')\n",
    "axes[0, 0].set_title('Exponential Distribution Sampling')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Beta distribution\n",
    "axes[1, 0].hist(beta_rejection, bins=30, alpha=0.7, density=True, label='Rejection Sampling', color='blue')\n",
    "axes[1, 0].hist(beta_direct, bins=30, alpha=0.7, density=True, label='Direct Sampling', color='red')\n",
    "x_beta = np.linspace(0, 1, 100)\n",
    "axes[1, 0].plot(x_beta, stats.beta.pdf(x_beta, 2, 3), 'k-', linewidth=2, label='True PDF')\n",
    "axes[1, 0].set_title('Beta Distribution Sampling')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Normal distribution\n",
    "axes[2, 0].hist(normal_mcmc, bins=30, alpha=0.7, density=True, label='MCMC Sampling', color='blue')\n",
    "axes[2, 0].hist(normal_direct, bins=30, alpha=0.7, density=True, label='Direct Sampling', color='red')\n",
    "x_norm = np.linspace(-2, 6, 100)\n",
    "axes[2, 0].plot(x_norm, stats.norm.pdf(x_norm, 2, 1.5), 'k-', linewidth=2, label='True PDF')\n",
    "axes[2, 0].set_title('Normal Distribution Sampling')\n",
    "axes[2, 0].legend()\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# MCMC trace plot\n",
    "axes[0, 1].plot(normal_mcmc[:200])\n",
    "axes[0, 1].set_title('MCMC Trace Plot (First 200 samples)')\n",
    "axes[0, 1].set_xlabel('Iteration')\n",
    "axes[0, 1].set_ylabel('Sample Value')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Autocorrelation\n",
    "def autocorrelation(x, max_lag=50):\n",
    "    n = len(x)\n",
    "    x = x - np.mean(x)\n",
    "    autocorr = np.correlate(x, x, mode='full')\n",
    "    autocorr = autocorr[n-1:n-1+max_lag]\n",
    "    return autocorr / autocorr[0]\n",
    "\n",
    "lags = range(50)\n",
    "autocorr = autocorrelation(normal_mcmc)\n",
    "axes[1, 1].plot(lags, autocorr)\n",
    "axes[1, 1].axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "axes[1, 1].set_title('MCMC Autocorrelation')\n",
    "axes[1, 1].set_xlabel('Lag')\n",
    "axes[1, 1].set_ylabel('Autocorrelation')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plots for comparison\n",
    "from scipy import stats as scipy_stats\n",
    "scipy_stats.probplot(normal_mcmc, dist=\"norm\", sparams=(2, 1.5), plot=axes[2, 1])\n",
    "axes[2, 1].set_title('Q-Q Plot: MCMC vs True Normal')\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"Sampling Method Comparison:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Exponential (Î»=1.5):\")\n",
    "print(f\"  True mean: {1/1.5:.3f}, Inverse transform: {np.mean(exp_inverse):.3f}, Direct: {np.mean(exp_direct):.3f}\")\n",
    "print(f\"\\nBeta (Î±=2, Î²=3):\")\n",
    "print(f\"  True mean: {2/(2+3):.3f}, Rejection: {np.mean(beta_rejection):.3f}, Direct: {np.mean(beta_direct):.3f}\")\n",
    "print(f\"\\nNormal (Î¼=2, Ïƒ=1.5):\")\n",
    "print(f\"  True mean: 2.000, MCMC: {np.mean(normal_mcmc):.3f}, Direct: {np.mean(normal_direct):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Metrics for Generative Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kl_divergence(p, q, epsilon=1e-10):\n",
    "    \"\"\"Calculate KL divergence between two discrete distributions\"\"\"\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    p = p + epsilon\n",
    "    q = q + epsilon\n",
    "    \n",
    "    # Normalize to ensure they sum to 1\n",
    "    p = p / np.sum(p)\n",
    "    q = q / np.sum(q)\n",
    "    \n",
    "    return np.sum(p * np.log(p / q))\n",
    "\n",
    "def calculate_js_divergence(p, q):\n",
    "    \"\"\"Calculate Jensen-Shannon divergence\"\"\"\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * calculate_kl_divergence(p, m) + 0.5 * calculate_kl_divergence(q, m)\n",
    "\n",
    "def wasserstein_distance_1d(x, y):\n",
    "    \"\"\"Calculate 1D Wasserstein distance (Earth Mover's Distance)\"\"\"\n",
    "    x_sorted = np.sort(x)\n",
    "    y_sorted = np.sort(y)\n",
    "    \n",
    "    # Make same length by interpolation\n",
    "    n = min(len(x_sorted), len(y_sorted))\n",
    "    x_interp = np.interp(np.linspace(0, 1, n), np.linspace(0, 1, len(x_sorted)), x_sorted)\n",
    "    y_interp = np.interp(np.linspace(0, 1, n), np.linspace(0, 1, len(y_sorted)), y_sorted)\n",
    "    \n",
    "    return np.mean(np.abs(x_interp - y_interp))\n",
    "\n",
    "# Generate true and synthetic data\n",
    "np.random.seed(42)\n",
    "true_data = np.random.normal(0, 1, 1000)\n",
    "\n",
    "# Different quality synthetic datasets\n",
    "good_synthetic = np.random.normal(0.1, 1.05, 1000)  # Close to true\n",
    "bad_synthetic = np.random.normal(0.5, 1.5, 1000)    # Further from true\n",
    "very_bad_synthetic = np.random.exponential(1, 1000)  # Different distribution\n",
    "\n",
    "# Calculate histograms for discrete distributions\n",
    "bins = np.linspace(-4, 4, 50)\n",
    "true_hist, _ = np.histogram(true_data, bins=bins, density=True)\n",
    "good_hist, _ = np.histogram(good_synthetic, bins=bins, density=True)\n",
    "bad_hist, _ = np.histogram(bad_synthetic, bins=bins, density=True)\n",
    "very_bad_hist, _ = np.histogram(very_bad_synthetic, bins=bins, density=True)\n",
    "\n",
    "# Normalize histograms\n",
    "true_hist = true_hist / np.sum(true_hist)\n",
    "good_hist = good_hist / np.sum(good_hist)\n",
    "bad_hist = bad_hist / np.sum(bad_hist)\n",
    "very_bad_hist = very_bad_hist / np.sum(very_bad_hist)\n",
    "\n",
    "# Calculate metrics\n",
    "datasets = [\n",
    "    (\"Good Synthetic\", good_synthetic, good_hist),\n",
    "    (\"Bad Synthetic\", bad_synthetic, bad_hist),\n",
    "    (\"Very Bad Synthetic\", very_bad_synthetic, very_bad_hist)\n",
    "]\n",
    "\n",
    "print(\"Generative Model Evaluation Metrics:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Dataset':<20} {'KL Div':<10} {'JS Div':<10} {'Wasserstein':<12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for name, data, hist in datasets:\n",
    "    kl_div = calculate_kl_divergence(true_hist, hist)\n",
    "    js_div = calculate_js_divergence(true_hist, hist)\n",
    "    wasserstein = wasserstein_distance_1d(true_data, data)\n",
    "    \n",
    "    print(f\"{name:<20} {kl_div:<10.4f} {js_div:<10.4f} {wasserstein:<12.4f}\")\n",
    "\n",
    "# Visualize distributions and metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot distributions\n",
    "x_range = np.linspace(-4, 6, 100)\n",
    "axes[0, 0].hist(true_data, bins=30, alpha=0.7, density=True, label='True Data', color='blue')\n",
    "axes[0, 0].hist(good_synthetic, bins=30, alpha=0.7, density=True, label='Good Synthetic', color='green')\n",
    "axes[0, 0].set_title('True vs Good Synthetic Data')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].hist(true_data, bins=30, alpha=0.7, density=True, label='True Data', color='blue')\n",
    "axes[0, 1].hist(bad_synthetic, bins=30, alpha=0.7, density=True, label='Bad Synthetic', color='orange')\n",
    "axes[0, 1].set_title('True vs Bad Synthetic Data')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].hist(true_data, bins=30, alpha=0.7, density=True, label='True Data', color='blue')\n",
    "axes[1, 0].hist(very_bad_synthetic, bins=30, alpha=0.7, density=True, label='Very Bad Synthetic', color='red')\n",
    "axes[1, 0].set_title('True vs Very Bad Synthetic Data')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Metric comparison\n",
    "metrics_data = {\n",
    "    'KL Divergence': [calculate_kl_divergence(true_hist, hist) for _, _, hist in datasets],\n",
    "    'JS Divergence': [calculate_js_divergence(true_hist, hist) for _, _, hist in datasets],\n",
    "    'Wasserstein': [wasserstein_distance_1d(true_data, data) for _, data, _ in datasets]\n",
    "}\n",
    "\n",
    "x_pos = np.arange(len(datasets))\n",
    "width = 0.25\n",
    "\n",
    "for i, (metric, values) in enumerate(metrics_data.items()):\n",
    "    axes[1, 1].bar(x_pos + i * width, values, width, label=metric, alpha=0.8)\n",
    "\n",
    "axes[1, 1].set_xlabel('Synthetic Datasets')\n",
    "axes[1, 1].set_ylabel('Metric Value')\n",
    "axes[1, 1].set_title('Evaluation Metrics Comparison')\n",
    "axes[1, 1].set_xticks(x_pos + width)\n",
    "axes[1, 1].set_xticklabels([name for name, _, _ in datasets], rotation=45)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "### Mathematical Foundations:\n",
    "1. **Probability Distributions**: Core of generative modeling\n",
    "2. **Maximum Likelihood Estimation**: Parameter learning principle\n",
    "3. **Latent Variable Models**: Hidden structure in data\n",
    "4. **Sampling Methods**: Different ways to generate from distributions\n",
    "\n",
    "### Practical Insights:\n",
    "1. **Model Selection**: Choose appropriate model for data structure\n",
    "2. **Evaluation**: Multiple metrics provide different perspectives\n",
    "3. **Trade-offs**: Quality vs. diversity vs. computational cost\n",
    "4. **Convergence**: Iterative algorithms need careful monitoring\n",
    "\n",
    "### Implementation Skills:\n",
    "1. **From Scratch**: Understanding underlying mathematics\n",
    "2. **Library Usage**: Leveraging existing implementations\n",
    "3. **Evaluation**: Proper assessment of generative quality\n",
    "4. **Visualization**: Understanding model behavior\n",
    "\n",
    "## ðŸ“ Exercises\n",
    "\n",
    "### Beginner:\n",
    "1. Implement a simple 1D Gaussian mixture model\n",
    "2. Compare different sampling methods for the same distribution\n",
    "3. Visualize the effect of different GMM parameters\n",
    "\n",
    "### Intermediate:\n",
    "1. Implement the full EM algorithm for GMM\n",
    "2. Create a factor analysis model (linear latent variable model)\n",
    "3. Implement different MCMC samplers (Gibbs, Hamiltonian Monte Carlo)\n",
    "\n",
    "### Advanced:\n",
    "1. Build a hierarchical Bayesian model\n",
    "2. Implement variational inference for a latent variable model\n",
    "3. Compare different divergence measures on real datasets\n",
    "\n",
    "## ðŸ”— Next Steps\n",
    "\n",
    "Now that you understand the fundamentals, you're ready to explore:\n",
    "1. **Variational Autoencoders (VAEs)**: Neural latent variable models\n",
    "2. **Generative Adversarial Networks (GANs)**: Adversarial training\n",
    "3. **Autoregressive Models**: Sequential generation\n",
    "4. **Diffusion Models**: Denoising-based generation\n",
    "\n",
    "**Next Notebook**: [Types of Generative Models](./02_types_of_models.ipynb) â†’"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}