{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Fundamentals: Mathematical Foundation and Implementation\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the mathematical derivation of VAEs\n",
    "- Learn the reparameterization trick\n",
    "- Implement a basic VAE from scratch\n",
    "- Analyze the Evidence Lower Bound (ELBO)\n",
    "- Compare VAE with standard autoencoders\n",
    "\n",
    "## What Makes VAEs Special?\n",
    "\n",
    "Unlike standard autoencoders that learn deterministic mappings, VAEs learn probabilistic encodings. This enables:\n",
    "1. **Generation**: Sample new data from the learned distribution\n",
    "2. **Interpolation**: Smooth transitions in latent space\n",
    "3. **Uncertainty**: Quantify confidence in representations\n",
    "4. **Regularization**: Structured latent space through KL divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set style and random seeds\n",
    "plt.style.use('seaborn-v0_8')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mathematical Foundation\n",
    "\n",
    "### The Generative Process\n",
    "VAEs model data generation as:\n",
    "1. Sample latent code: $z \\sim p(z)$ (prior)\n",
    "2. Generate data: $x \\sim p(x|z)$ (likelihood)\n",
    "\n",
    "### The Inference Problem\n",
    "We want to maximize: $\\log p(x) = \\log \\int p(x|z)p(z)dz$\n",
    "\n",
    "Since this integral is intractable, we use variational inference with approximate posterior $q(z|x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the VAE framework\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Prior distribution p(z)\n",
    "z_samples = np.random.normal(0, 1, 1000)\n",
    "axes[0].hist(z_samples, bins=30, alpha=0.7, density=True, color='blue')\n",
    "axes[0].set_title('Prior p(z) ~ N(0,1)')\n",
    "axes[0].set_xlabel('z')\n",
    "axes[0].set_ylabel('Density')\n",
    "\n",
    "# Approximate posterior q(z|x)\n",
    "mu, sigma = 0.5, 0.8\n",
    "z_posterior = np.random.normal(mu, sigma, 1000)\n",
    "axes[1].hist(z_posterior, bins=30, alpha=0.7, density=True, color='red')\n",
    "axes[1].set_title(f'Posterior q(z|x) ~ N({mu},{sigma}¬≤)')\n",
    "axes[1].set_xlabel('z')\n",
    "axes[1].set_ylabel('Density')\n",
    "\n",
    "# KL divergence visualization\n",
    "z_range = np.linspace(-3, 4, 100)\n",
    "prior_pdf = (1/np.sqrt(2*np.pi)) * np.exp(-0.5 * z_range**2)\n",
    "posterior_pdf = (1/(sigma*np.sqrt(2*np.pi))) * np.exp(-0.5 * ((z_range - mu)/sigma)**2)\n",
    "\n",
    "axes[2].plot(z_range, prior_pdf, label='p(z)', color='blue')\n",
    "axes[2].plot(z_range, posterior_pdf, label='q(z|x)', color='red')\n",
    "axes[2].fill_between(z_range, 0, np.minimum(prior_pdf, posterior_pdf), alpha=0.3, color='green')\n",
    "axes[2].set_title('KL Divergence KL(q||p)')\n",
    "axes[2].set_xlabel('z')\n",
    "axes[2].set_ylabel('Density')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate KL divergence analytically for Gaussians\n",
    "kl_div = 0.5 * (sigma**2 + mu**2 - 1 - 2*np.log(sigma))\n",
    "print(f\"KL divergence between N({mu},{sigma}¬≤) and N(0,1): {kl_div:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evidence Lower Bound (ELBO) Derivation\n",
    "\n",
    "Starting from the log-likelihood:\n",
    "\n",
    "$$\\log p(x) = \\mathbb{E}_{q(z|x)}[\\log p(x)] = \\mathbb{E}_{q(z|x)}\\left[\\log \\frac{p(x,z)}{p(z|x)}\\right]$$\n",
    "\n",
    "$$= \\mathbb{E}_{q(z|x)}\\left[\\log \\frac{p(x,z)q(z|x)}{p(z|x)q(z|x)}\\right]$$\n",
    "\n",
    "$$= \\mathbb{E}_{q(z|x)}\\left[\\log \\frac{p(x,z)}{q(z|x)}\\right] + \\mathbb{E}_{q(z|x)}\\left[\\log \\frac{q(z|x)}{p(z|x)}\\right]$$\n",
    "\n",
    "$$= \\text{ELBO} + \\text{KL}(q(z|x) || p(z|x))$$\n",
    "\n",
    "Since KL ‚â• 0, we have: $\\log p(x) \\geq \\text{ELBO}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_elbo_components(x, mu, logvar, recon_x):\n",
    "    \"\"\"\n",
    "    Compute ELBO components for analysis\n",
    "    \"\"\"\n",
    "    # Reconstruction term: E[log p(x|z)]\n",
    "    recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    \n",
    "    # KL divergence: KL(q(z|x) || p(z))\n",
    "    # For q(z|x) = N(Œº, œÉ¬≤I) and p(z) = N(0, I):\n",
    "    # KL = 0.5 * sum(œÉ¬≤ + Œº¬≤ - 1 - log(œÉ¬≤))\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    # ELBO = E[log p(x|z)] - KL(q(z|x) || p(z))\n",
    "    # We minimize the negative ELBO\n",
    "    elbo = -recon_loss - kl_div\n",
    "    \n",
    "    return {\n",
    "        'elbo': elbo.item(),\n",
    "        'reconstruction_loss': recon_loss.item(),\n",
    "        'kl_divergence': kl_div.item(),\n",
    "        'total_loss': (-elbo).item()  # What we actually minimize\n",
    "    }\n",
    "\n",
    "# Demonstrate ELBO computation with dummy data\n",
    "batch_size, latent_dim = 32, 10\n",
    "x_dummy = torch.rand(batch_size, 784)  # Flattened 28x28 images\n",
    "mu_dummy = torch.randn(batch_size, latent_dim)\n",
    "logvar_dummy = torch.randn(batch_size, latent_dim)\n",
    "recon_dummy = torch.sigmoid(torch.randn(batch_size, 784))\n",
    "\n",
    "elbo_components = compute_elbo_components(x_dummy, mu_dummy, logvar_dummy, recon_dummy)\n",
    "\n",
    "print(\"ELBO Components (dummy data):\")\n",
    "for key, value in elbo_components.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Reparameterization Trick\n",
    "\n",
    "**Problem**: We can't backpropagate through stochastic sampling $z \\sim q(z|x)$\n",
    "\n",
    "**Solution**: Reparameterize as $z = \\mu + \\sigma \\odot \\epsilon$ where $\\epsilon \\sim N(0,I)$\n",
    "\n",
    "This makes the stochasticity independent of the parameters we're optimizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu, logvar):\n",
    "    \"\"\"\n",
    "    Reparameterization trick: z = Œº + œÉ * Œµ, where Œµ ~ N(0,1)\n",
    "    \"\"\"\n",
    "    std = torch.exp(0.5 * logvar)  # œÉ = exp(0.5 * log(œÉ¬≤))\n",
    "    eps = torch.randn_like(std)    # Œµ ~ N(0,1)\n",
    "    return mu + eps * std\n",
    "\n",
    "def reparameterize_no_trick(mu, logvar):\n",
    "    \"\"\"\n",
    "    Direct sampling (doesn't allow gradients)\n",
    "    \"\"\"\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    return torch.normal(mu, std)\n",
    "\n",
    "# Demonstrate gradient flow\n",
    "mu = torch.tensor([0.0, 1.0], requires_grad=True)\n",
    "logvar = torch.tensor([0.0, 0.5], requires_grad=True)\n",
    "\n",
    "print(\"Testing gradient flow:\")\n",
    "\n",
    "# With reparameterization trick\n",
    "z_reparam = reparameterize(mu, logvar)\n",
    "loss_reparam = z_reparam.sum()\n",
    "loss_reparam.backward()\n",
    "print(f\"With reparameterization - mu.grad: {mu.grad}\")\n",
    "\n",
    "# Reset gradients\n",
    "mu.grad = None\n",
    "logvar.grad = None\n",
    "\n",
    "# Without reparameterization (this would fail in practice)\n",
    "try:\n",
    "    z_direct = reparameterize_no_trick(mu, logvar)\n",
    "    loss_direct = z_direct.sum()\n",
    "    loss_direct.backward()\n",
    "    print(f\"Without reparameterization - mu.grad: {mu.grad}\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error without reparameterization: {e}\")\n",
    "\n",
    "# Visualize the effect of reparameterization\n",
    "mu_test = torch.tensor([0.0])\n",
    "logvar_test = torch.tensor([1.0])\n",
    "\n",
    "samples_reparam = [reparameterize(mu_test, logvar_test).item() for _ in range(1000)]\n",
    "samples_direct = [torch.normal(mu_test, torch.exp(0.5 * logvar_test)).item() for _ in range(1000)]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].hist(samples_reparam, bins=30, alpha=0.7, density=True, label='Reparameterized')\n",
    "axes[0].set_title('Reparameterized Sampling')\n",
    "axes[0].set_xlabel('z')\n",
    "axes[0].set_ylabel('Density')\n",
    "\n",
    "axes[1].hist(samples_direct, bins=30, alpha=0.7, density=True, label='Direct', color='orange')\n",
    "axes[1].set_title('Direct Sampling')\n",
    "axes[1].set_xlabel('z')\n",
    "axes[1].set_ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Both methods produce the same distribution:\")\n",
    "print(f\"Reparameterized - Mean: {np.mean(samples_reparam):.3f}, Std: {np.std(samples_reparam):.3f}\")\n",
    "print(f\"Direct - Mean: {np.mean(samples_direct):.3f}, Std: {np.std(samples_direct):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic VAE Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicVAE(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):\n",
    "        super(BasicVAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Latent space parameters\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()  # Output in [0,1] for MNIST\n",
    "        )\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode input to latent parameters\"\"\"\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Reparameterization trick\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode latent code to reconstruction\"\"\"\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mu, logvar\n",
    "    \n",
    "    def sample(self, num_samples, device):\n",
    "        \"\"\"Generate new samples from prior\"\"\"\n",
    "        z = torch.randn(num_samples, self.latent_dim).to(device)\n",
    "        return self.decode(z)\n",
    "\n",
    "# Initialize model\n",
    "model = BasicVAE(input_dim=784, hidden_dim=400, latent_dim=20).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Print model architecture\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loss Function Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(recon_x, x, mu, logvar, beta=1.0):\n",
    "    \"\"\"\n",
    "    VAE loss function: -ELBO = Reconstruction Loss + Œ≤ * KL Divergence\n",
    "    \n",
    "    Args:\n",
    "        recon_x: Reconstructed input\n",
    "        x: Original input\n",
    "        mu: Mean of latent distribution\n",
    "        logvar: Log variance of latent distribution\n",
    "        beta: Weight for KL divergence (Œ≤-VAE)\n",
    "    \"\"\"\n",
    "    # Reconstruction loss (negative log-likelihood)\n",
    "    # For binary data, use BCE; for continuous data, use MSE\n",
    "    recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    \n",
    "    # KL divergence between q(z|x) and p(z) = N(0,I)\n",
    "    # KL(N(Œº,œÉ¬≤)||N(0,1)) = 0.5 * (œÉ¬≤ + Œº¬≤ - 1 - log(œÉ¬≤))\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = recon_loss + beta * kl_loss\n",
    "    \n",
    "    return total_loss, recon_loss, kl_loss\n",
    "\n",
    "def vae_loss_mse(recon_x, x, mu, logvar, beta=1.0):\n",
    "    \"\"\"VAE loss with MSE reconstruction (for continuous data)\"\"\"\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + beta * kl_loss, recon_loss, kl_loss\n",
    "\n",
    "# Test loss function with dummy data\n",
    "x_test = torch.rand(10, 784).to(device)\n",
    "recon_test, mu_test, logvar_test = model(x_test)\n",
    "\n",
    "loss, recon_loss, kl_loss = vae_loss(recon_test, x_test, mu_test, logvar_test)\n",
    "\n",
    "print(f\"Loss components (test):\")\n",
    "print(f\"Total loss: {loss.item():.4f}\")\n",
    "print(f\"Reconstruction loss: {recon_loss.item():.4f}\")\n",
    "print(f\"KL divergence: {kl_loss.item():.4f}\")\n",
    "print(f\"KL per dimension: {kl_loss.item() / (10 * 20):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(-1))  # Flatten to 784 dimensions\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Visualize some samples\n",
    "sample_batch, sample_labels = next(iter(train_loader))\n",
    "print(f\"Batch shape: {sample_batch.shape}\")\n",
    "print(f\"Data range: [{sample_batch.min():.3f}, {sample_batch.max():.3f}]\")\n",
    "\n",
    "# Plot sample images\n",
    "fig, axes = plt.subplots(2, 8, figsize=(12, 4))\n",
    "for i in range(8):\n",
    "    # Original\n",
    "    axes[0, i].imshow(sample_batch[i].view(28, 28), cmap='gray')\n",
    "    axes[0, i].set_title(f'Label: {sample_labels[i]}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Histogram of pixel values\n",
    "    axes[1, i].hist(sample_batch[i].numpy(), bins=20, alpha=0.7)\n",
    "    axes[1, i].set_title('Pixel Distribution')\n",
    "    axes[1, i].set_xlabel('Pixel Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(model, train_loader, optimizer, epoch, beta=1.0):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_recon_loss = 0\n",
    "    train_kl_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, recon_loss, kl_loss = vae_loss(recon_batch, data, mu, logvar, beta)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_recon_loss += recon_loss.item()\n",
    "        train_kl_loss += kl_loss.item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\t'\n",
    "                  f'Loss: {loss.item() / len(data):.6f}')\n",
    "    \n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_recon = train_recon_loss / len(train_loader.dataset)\n",
    "    avg_kl = train_kl_loss / len(train_loader.dataset)\n",
    "    \n",
    "    print(f'====> Epoch: {epoch} Average loss: {avg_loss:.4f} '\n",
    "          f'Recon: {avg_recon:.4f} KL: {avg_kl:.4f}')\n",
    "    \n",
    "    return avg_loss, avg_recon, avg_kl\n",
    "\n",
    "def test_vae(model, test_loader, beta=1.0):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_recon_loss = 0\n",
    "    test_kl_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            \n",
    "            loss, recon_loss, kl_loss = vae_loss(recon_batch, data, mu, logvar, beta)\n",
    "            test_loss += loss.item()\n",
    "            test_recon_loss += recon_loss.item()\n",
    "            test_kl_loss += kl_loss.item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_recon_loss /= len(test_loader.dataset)\n",
    "    test_kl_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    print(f'====> Test set loss: {test_loss:.4f} '\n",
    "          f'Recon: {test_recon_loss:.4f} KL: {test_kl_loss:.4f}')\n",
    "    \n",
    "    return test_loss, test_recon_loss, test_kl_loss\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "recon_losses = []\n",
    "kl_losses = []\n",
    "\n",
    "print(\"Starting VAE training...\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Beta annealing (gradually increase KL weight)\n",
    "    beta = min(1.0, epoch / 5.0)\n",
    "    \n",
    "    train_loss, train_recon, train_kl = train_vae(model, train_loader, optimizer, epoch, beta)\n",
    "    test_loss, test_recon, test_kl = test_vae(model, test_loader, beta)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    recon_losses.append(train_recon)\n",
    "    kl_losses.append(train_kl)\n",
    "    \n",
    "    print(f\"Beta: {beta:.3f}\\n\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyzing Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Total loss\n",
    "axes[0, 0].plot(train_losses, label='Train', marker='o')\n",
    "axes[0, 0].plot(test_losses, label='Test', marker='s')\n",
    "axes[0, 0].set_title('Total Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss components\n",
    "axes[0, 1].plot(recon_losses, label='Reconstruction', marker='o', color='blue')\n",
    "axes[0, 1].plot(kl_losses, label='KL Divergence', marker='s', color='red')\n",
    "axes[0, 1].set_title('Loss Components')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# KL/Reconstruction ratio\n",
    "kl_recon_ratio = np.array(kl_losses) / np.array(recon_losses)\n",
    "axes[1, 0].plot(kl_recon_ratio, marker='o', color='green')\n",
    "axes[1, 0].set_title('KL/Reconstruction Ratio')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Ratio')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# KL per latent dimension\n",
    "kl_per_dim = np.array(kl_losses) / model.latent_dim\n",
    "axes[1, 1].plot(kl_per_dim, marker='o', color='purple')\n",
    "axes[1, 1].axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='KL=1 per dim')\n",
    "axes[1, 1].set_title('KL Divergence per Latent Dimension')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('KL per Dimension')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training metrics:\")\n",
    "print(f\"Total loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Reconstruction loss: {recon_losses[-1]:.4f}\")\n",
    "print(f\"KL divergence: {kl_losses[-1]:.4f}\")\n",
    "print(f\"KL per dimension: {kl_per_dim[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluating Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstructions(model, test_loader, num_samples=8):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data = data.to(device)\n",
    "            recon, mu, logvar = model(data)\n",
    "            \n",
    "            # Plot original vs reconstruction\n",
    "            fig, axes = plt.subplots(3, num_samples, figsize=(15, 6))\n",
    "            \n",
    "            for i in range(num_samples):\n",
    "                # Original\n",
    "                axes[0, i].imshow(data[i].cpu().view(28, 28), cmap='gray')\n",
    "                axes[0, i].set_title(f'Original (Label: {labels[i]})')\n",
    "                axes[0, i].axis('off')\n",
    "                \n",
    "                # Reconstruction\n",
    "                axes[1, i].imshow(recon[i].cpu().view(28, 28), cmap='gray')\n",
    "                axes[1, i].set_title('Reconstruction')\n",
    "                axes[1, i].axis('off')\n",
    "                \n",
    "                # Difference\n",
    "                diff = torch.abs(data[i] - recon[i]).cpu().view(28, 28)\n",
    "                im = axes[2, i].imshow(diff, cmap='hot')\n",
    "                axes[2, i].set_title(f'Diff (MSE: {F.mse_loss(recon[i], data[i]):.3f})')\n",
    "                axes[2, i].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            break\n",
    "\n",
    "visualize_reconstructions(model, test_loader)\n",
    "\n",
    "# Compute reconstruction statistics\n",
    "def compute_reconstruction_stats(model, test_loader):\n",
    "    model.eval()\n",
    "    mse_errors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.to(device)\n",
    "            recon, _, _ = model(data)\n",
    "            \n",
    "            # Compute MSE for each sample\n",
    "            mse = F.mse_loss(recon, data, reduction='none').mean(dim=1)\n",
    "            mse_errors.extend(mse.cpu().numpy())\n",
    "    \n",
    "    return np.array(mse_errors)\n",
    "\n",
    "mse_errors = compute_reconstruction_stats(model, test_loader)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(mse_errors, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(np.mean(mse_errors), color='red', linestyle='--', \n",
    "           label=f'Mean: {np.mean(mse_errors):.4f}')\n",
    "plt.axvline(np.median(mse_errors), color='green', linestyle='--', \n",
    "           label=f'Median: {np.median(mse_errors):.4f}')\n",
    "plt.title('Distribution of Reconstruction Errors')\n",
    "plt.xlabel('MSE')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Reconstruction statistics:\")\n",
    "print(f\"Mean MSE: {np.mean(mse_errors):.6f}\")\n",
    "print(f\"Std MSE: {np.std(mse_errors):.6f}\")\n",
    "print(f\"Min MSE: {np.min(mse_errors):.6f}\")\n",
    "print(f\"Max MSE: {np.max(mse_errors):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generating New Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, num_samples=16):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Sample from prior N(0,I)\n",
    "        z = torch.randn(num_samples, model.latent_dim).to(device)\n",
    "        samples = model.decode(z)\n",
    "        \n",
    "        # Plot generated samples\n",
    "        fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            axes[i].imshow(samples[i].cpu().view(28, 28), cmap='gray')\n",
    "            axes[i].set_title(f'Sample {i+1}')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.suptitle('Generated Samples from VAE')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return samples\n",
    "\n",
    "generated_samples = generate_samples(model, 16)\n",
    "\n",
    "# Compare with real samples\n",
    "real_samples, _ = next(iter(test_loader))\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(12, 4))\n",
    "\n",
    "for i in range(8):\n",
    "    # Real samples\n",
    "    axes[0, i].imshow(real_samples[i].view(28, 28), cmap='gray')\n",
    "    axes[0, i].set_title('Real')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Generated samples\n",
    "    axes[1, i].imshow(generated_samples[i].cpu().view(28, 28), cmap='gray')\n",
    "    axes[1, i].set_title('Generated')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('Real vs Generated Samples')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### Mathematical Insights:\n",
    "1. **ELBO Maximization**: VAEs maximize a lower bound on the log-likelihood\n",
    "2. **Reparameterization Trick**: Enables gradient flow through stochastic nodes\n",
    "3. **KL Regularization**: Forces latent codes to match the prior distribution\n",
    "4. **Trade-off**: Balance between reconstruction quality and latent regularity\n",
    "\n",
    "### Implementation Insights:\n",
    "1. **Beta Annealing**: Gradually increase KL weight for stable training\n",
    "2. **Loss Components**: Monitor reconstruction and KL losses separately\n",
    "3. **Latent Dimensions**: More dimensions = more capacity but harder optimization\n",
    "4. **Architecture**: Encoder/decoder symmetry often works well\n",
    "\n",
    "### Practical Insights:\n",
    "1. **Generation Quality**: VAEs produce smoother but blurrier samples than GANs\n",
    "2. **Latent Space**: Continuous and interpolatable latent representations\n",
    "3. **Training Stability**: More stable than GANs, less prone to mode collapse\n",
    "4. **Applications**: Great for representation learning and anomaly detection\n",
    "\n",
    "## üìù Exercises\n",
    "\n",
    "### Beginner:\n",
    "1. Modify the latent dimension and observe the effect on generation quality\n",
    "2. Try different beta values and analyze the reconstruction-regularization trade-off\n",
    "3. Implement VAE with MSE loss instead of BCE\n",
    "\n",
    "### Intermediate:\n",
    "1. Add batch normalization to the encoder and decoder\n",
    "2. Implement learning rate scheduling\n",
    "3. Create a convolutional VAE for CIFAR-10\n",
    "\n",
    "### Advanced:\n",
    "1. Implement Œ≤-VAE with automatic beta tuning\n",
    "2. Add skip connections to the decoder\n",
    "3. Implement hierarchical VAE with multiple latent levels\n",
    "\n",
    "## üîó Next Steps\n",
    "\n",
    "Now that you understand VAE fundamentals, you're ready to explore:\n",
    "1. **Advanced VAE Variants**: Œ≤-VAE, WAE, VQ-VAE\n",
    "2. **Latent Space Analysis**: Disentanglement and interpolation\n",
    "3. **Applications**: Anomaly detection, data augmentation\n",
    "4. **Comparisons**: VAE vs GAN trade-offs\n",
    "\n",
    "**Next Notebook**: [VAE Architecture Design](./02_vae_architectures.ipynb) ‚Üí"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}