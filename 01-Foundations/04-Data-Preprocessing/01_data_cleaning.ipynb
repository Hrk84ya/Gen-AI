{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Fundamentals\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand common data quality issues\n",
    "- Learn techniques for handling missing data\n",
    "- Master data type conversions\n",
    "- Remove duplicates and outliers\n",
    "\n",
    "## Prerequisites\n",
    "- Basic Python knowledge\n",
    "- Pandas fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Sample Messy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messy dataset for demonstration\n",
    "np.random.seed(42)\n",
    "\n",
    "data = {\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Alice', 'Eve', None, 'Frank', 'Grace'],\n",
    "    'age': [25, 30, None, 25, 35, 28, 40, 22],\n",
    "    'salary': [50000, 60000, 70000, 50000, None, 55000, 80000, 45000],\n",
    "    'department': ['IT', 'HR', 'IT', 'IT', 'Finance', 'HR', 'IT', 'Finance'],\n",
    "    'join_date': ['2020-01-15', '2019-03-20', '2021-07-10', '2020-01-15', \n",
    "                  '2018-11-05', '2020-09-12', '2017-04-18', '2022-02-28']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "print(f\"\\nDataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data info\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nDuplicate Rows:\")\n",
    "print(f\"Number of duplicates: {df.duplicated().sum()}\")\n",
    "\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Drop rows with missing values\n",
    "df_dropped = df.dropna()\n",
    "print(\"After dropping missing values:\")\n",
    "print(df_dropped)\n",
    "print(f\"Shape: {df_dropped.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Fill missing values\n",
    "df_filled = df.copy()\n",
    "\n",
    "# Fill missing names with 'Unknown'\n",
    "df_filled['name'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Fill missing age with median\n",
    "df_filled['age'].fillna(df_filled['age'].median(), inplace=True)\n",
    "\n",
    "# Fill missing salary with mean\n",
    "df_filled['salary'].fillna(df_filled['salary'].mean(), inplace=True)\n",
    "\n",
    "print(\"After filling missing values:\")\n",
    "print(df_filled)\n",
    "print(f\"\\nMissing values: {df_filled.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "df_no_duplicates = df_filled.drop_duplicates()\n",
    "print(\"After removing duplicates:\")\n",
    "print(df_no_duplicates)\n",
    "print(f\"Shape: {df_no_duplicates.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Type Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert join_date to datetime\n",
    "df_clean = df_no_duplicates.copy()\n",
    "df_clean['join_date'] = pd.to_datetime(df_clean['join_date'])\n",
    "\n",
    "# Convert age to integer\n",
    "df_clean['age'] = df_clean['age'].astype(int)\n",
    "\n",
    "# Convert salary to integer\n",
    "df_clean['salary'] = df_clean['salary'].astype(int)\n",
    "\n",
    "print(\"Final cleaned dataset:\")\n",
    "print(df_clean)\n",
    "print(f\"\\nData types:\")\n",
    "print(df_clean.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Outlier Detection and Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using IQR method\n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Check for outliers in salary\n",
    "salary_outliers, lower, upper = detect_outliers_iqr(df_clean, 'salary')\n",
    "print(f\"Salary outliers (< {lower:.0f} or > {upper:.0f}):\")\n",
    "print(salary_outliers)\n",
    "\n",
    "# Visualize outliers\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(df_clean['salary'])\n",
    "plt.title('Salary Distribution (Box Plot)')\n",
    "plt.ylabel('Salary')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df_clean['salary'], bins=10, edgecolor='black')\n",
    "plt.title('Salary Distribution (Histogram)')\n",
    "plt.xlabel('Salary')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data ranges and constraints\n",
    "def validate_data(df):\n",
    "    issues = []\n",
    "    \n",
    "    # Check age range\n",
    "    if (df['age'] < 18).any() or (df['age'] > 100).any():\n",
    "        issues.append(\"Age values outside reasonable range (18-100)\")\n",
    "    \n",
    "    # Check salary range\n",
    "    if (df['salary'] < 0).any():\n",
    "        issues.append(\"Negative salary values found\")\n",
    "    \n",
    "    # Check for future join dates\n",
    "    if (df['join_date'] > pd.Timestamp.now()).any():\n",
    "        issues.append(\"Future join dates found\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "validation_issues = validate_data(df_clean)\n",
    "if validation_issues:\n",
    "    print(\"Data validation issues:\")\n",
    "    for issue in validation_issues:\n",
    "        print(f\"- {issue}\")\n",
    "else:\n",
    "    print(\"‚úÖ Data validation passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics\n",
    "print(\"Summary Statistics:\")\n",
    "print(df_clean.describe())\n",
    "\n",
    "print(\"\\nCategorical Data Summary:\")\n",
    "print(df_clean['department'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "1. **Always assess data quality first** - Check for missing values, duplicates, and data types\n",
    "2. **Choose appropriate strategies** - Different missing data strategies for different scenarios\n",
    "3. **Handle outliers carefully** - Understand if they're errors or valid extreme values\n",
    "4. **Validate your cleaning** - Ensure the cleaned data makes business sense\n",
    "5. **Document your process** - Keep track of all transformations for reproducibility\n",
    "\n",
    "## üìù Exercises\n",
    "\n",
    "1. Create a dataset with different types of missing data patterns\n",
    "2. Implement forward-fill and backward-fill strategies\n",
    "3. Compare different outlier detection methods (Z-score, IQR, Isolation Forest)\n",
    "4. Build a data cleaning pipeline function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}