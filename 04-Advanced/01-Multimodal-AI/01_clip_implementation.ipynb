{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP Implementation and Fine-tuning\n",
    "\n",
    "This notebook demonstrates how to use and fine-tune CLIP (Contrastive Language-Image Pre-training) for multimodal tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Pre-trained CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load CLIP model and processor\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Image-Text Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compute_similarity(image, texts):\n",
    "    \"\"\"Compute similarity between an image and multiple text descriptions\"\"\"\n",
    "    \n",
    "    # Process inputs\n",
    "    inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        probs = logits_per_image.softmax(dim=-1)\n",
    "    \n",
    "    return probs.cpu().numpy()[0]\n",
    "\n",
    "# Example usage\n",
    "# Load a sample image (you can replace with your own)\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7d/A_golden_retriever_sitting_in_the_snow.jpg/320px-A_golden_retriever_sitting_in_the_snow.jpg\"\n",
    "response = requests.get(url)\n",
    "image = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Define text descriptions\n",
    "texts = [\n",
    "    \"a photo of a dog\",\n",
    "    \"a photo of a cat\",\n",
    "    \"a photo of a car\",\n",
    "    \"a golden retriever in snow\"\n",
    "]\n",
    "\n",
    "# Compute similarities\n",
    "similarities = compute_similarity(image, texts)\n",
    "\n",
    "# Display results\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title('Input Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(len(texts)), similarities)\n",
    "plt.xticks(range(len(texts)), texts, rotation=45, ha='right')\n",
    "plt.ylabel('Similarity Score')\n",
    "plt.title('Text-Image Similarities')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for text, sim in zip(texts, similarities):\n",
    "    print(f\"{text}: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Zero-Shot Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def zero_shot_classification(image, class_names):\n",
    "    \"\"\"Perform zero-shot image classification using CLIP\"\"\"\n",
    "    \n",
    "    # Create text prompts\n",
    "    text_prompts = [f\"a photo of a {class_name}\" for class_name in class_names]\n",
    "    \n",
    "    # Compute similarities\n",
    "    similarities = compute_similarity(image, text_prompts)\n",
    "    \n",
    "    # Get prediction\n",
    "    predicted_idx = np.argmax(similarities)\n",
    "    predicted_class = class_names[predicted_idx]\n",
    "    confidence = similarities[predicted_idx]\n",
    "    \n",
    "    return predicted_class, confidence, similarities\n",
    "\n",
    "# Example: Animal classification\n",
    "animal_classes = ['dog', 'cat', 'bird', 'fish', 'horse', 'cow', 'elephant']\n",
    "\n",
    "predicted_class, confidence, all_similarities = zero_shot_classification(image, animal_classes)\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Confidence: {confidence:.3f}\")\n",
    "print(\"\\nAll class probabilities:\")\n",
    "for class_name, sim in zip(animal_classes, all_similarities):\n",
    "    print(f\"{class_name}: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Dataset for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class CustomMultimodalDataset(Dataset):\n",
    "    def __init__(self, image_paths, captions, processor, max_length=77):\n",
    "        self.image_paths = image_paths\n",
    "        self.captions = captions\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        caption = self.captions[idx]\n",
    "        \n",
    "        # Process image and text\n",
    "        inputs = self.processor(\n",
    "            text=caption,\n",
    "            images=image,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "# Example dataset creation (replace with your data)\n",
    "# image_paths = ['path/to/image1.jpg', 'path/to/image2.jpg', ...]\n",
    "# captions = ['caption for image 1', 'caption for image 2', ...]\n",
    "# dataset = CustomMultimodalDataset(image_paths, captions, processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine-tuning CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def contrastive_loss(logits, temperature=0.07):\n",
    "    \"\"\"Compute contrastive loss for CLIP training\"\"\"\n",
    "    # Normalize logits\n",
    "    logits = logits / temperature\n",
    "    \n",
    "    # Create labels (diagonal matrix)\n",
    "    batch_size = logits.shape[0]\n",
    "    labels = torch.arange(batch_size, device=logits.device)\n",
    "    \n",
    "    # Compute cross-entropy loss for both directions\n",
    "    loss_i2t = nn.CrossEntropyLoss()(logits, labels)\n",
    "    loss_t2i = nn.CrossEntropyLoss()(logits.T, labels)\n",
    "    \n",
    "    return (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "def train_clip(model, dataloader, num_epochs=5, learning_rate=1e-5):\n",
    "    \"\"\"Fine-tune CLIP model\"\"\"\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = contrastive_loss(outputs.logits_per_image)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example fine-tuning (uncomment when you have data)\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "# fine_tuned_model = train_clip(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Image Retrieval with Text Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def image_retrieval(query_text, image_database, top_k=5):\n",
    "    \"\"\"Retrieve most relevant images for a text query\"\"\"\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    for image_path in image_database:\n",
    "        # Load and process image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Compute similarity\n",
    "        sim = compute_similarity(image, [query_text])[0]\n",
    "        similarities.append((image_path, sim))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Example usage (replace with your image database)\n",
    "# image_database = ['path/to/image1.jpg', 'path/to/image2.jpg', ...]\n",
    "# query = \"a dog playing in the park\"\n",
    "# results = image_retrieval(query, image_database)\n",
    "# \n",
    "# print(f\"Top results for query: '{query}'\")\n",
    "# for i, (image_path, similarity) in enumerate(results):\n",
    "#     print(f\"{i+1}. {image_path}: {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multimodal Embeddings Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def extract_embeddings(images, texts):\n",
    "    \"\"\"Extract CLIP embeddings for images and texts\"\"\"\n",
    "    \n",
    "    image_embeddings = []\n",
    "    text_embeddings = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Process images\n",
    "        for image in images:\n",
    "            inputs = processor(images=image, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            image_features = model.get_image_features(**inputs)\n",
    "            image_embeddings.append(image_features.cpu().numpy())\n",
    "        \n",
    "        # Process texts\n",
    "        for text in texts:\n",
    "            inputs = processor(text=text, return_tensors=\"pt\", padding=True)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            text_features = model.get_text_features(**inputs)\n",
    "            text_embeddings.append(text_features.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(image_embeddings), np.vstack(text_embeddings)\n",
    "\n",
    "def visualize_embeddings(image_embeddings, text_embeddings, image_labels, text_labels):\n",
    "    \"\"\"Visualize embeddings using t-SNE\"\"\"\n",
    "    \n",
    "    # Combine embeddings\n",
    "    all_embeddings = np.vstack([image_embeddings, text_embeddings])\n",
    "    \n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    embeddings_2d = tsne.fit_transform(all_embeddings)\n",
    "    \n",
    "    # Split back\n",
    "    n_images = len(image_embeddings)\n",
    "    image_2d = embeddings_2d[:n_images]\n",
    "    text_2d = embeddings_2d[n_images:]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot images\n",
    "    plt.scatter(image_2d[:, 0], image_2d[:, 1], c='red', marker='o', s=100, alpha=0.7, label='Images')\n",
    "    for i, label in enumerate(image_labels):\n",
    "        plt.annotate(label, (image_2d[i, 0], image_2d[i, 1]), xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    # Plot texts\n",
    "    plt.scatter(text_2d[:, 0], text_2d[:, 1], c='blue', marker='s', s=100, alpha=0.7, label='Texts')\n",
    "    for i, label in enumerate(text_labels):\n",
    "        plt.annotate(label, (text_2d[i, 0], text_2d[i, 1]), xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.title('CLIP Embeddings Visualization')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage (replace with your data)\n",
    "# images = [image1, image2, image3, ...]  # PIL Images\n",
    "# texts = [\"text1\", \"text2\", \"text3\", ...]\n",
    "# image_labels = [\"img1\", \"img2\", \"img3\", ...]\n",
    "# text_labels = [\"txt1\", \"txt2\", \"txt3\", ...]\n",
    "# \n",
    "# img_emb, txt_emb = extract_embeddings(images, texts)\n",
    "# visualize_embeddings(img_emb, txt_emb, image_labels, text_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def recall_at_k(similarities, k=5):\n",
    "    \"\"\"Compute Recall@K for image-text retrieval\"\"\"\n",
    "    \n",
    "    n = similarities.shape[0]\n",
    "    recall_scores = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Get top-k most similar items\n",
    "        top_k_indices = np.argsort(similarities[i])[-k:]\n",
    "        \n",
    "        # Check if correct match is in top-k\n",
    "        recall = 1 if i in top_k_indices else 0\n",
    "        recall_scores.append(recall)\n",
    "    \n",
    "    return np.mean(recall_scores)\n",
    "\n",
    "def evaluate_retrieval(model, test_images, test_texts):\n",
    "    \"\"\"Evaluate image-text retrieval performance\"\"\"\n",
    "    \n",
    "    # Extract embeddings\n",
    "    image_embeddings, text_embeddings = extract_embeddings(test_images, test_texts)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    similarities = np.dot(image_embeddings, text_embeddings.T)\n",
    "    \n",
    "    # Compute metrics\n",
    "    recall_1 = recall_at_k(similarities, k=1)\n",
    "    recall_5 = recall_at_k(similarities, k=5)\n",
    "    recall_10 = recall_at_k(similarities, k=10)\n",
    "    \n",
    "    print(f\"Image-to-Text Retrieval:\")\n",
    "    print(f\"Recall@1: {recall_1:.3f}\")\n",
    "    print(f\"Recall@5: {recall_5:.3f}\")\n",
    "    print(f\"Recall@10: {recall_10:.3f}\")\n",
    "    \n",
    "    # Text-to-Image retrieval (transpose similarities)\n",
    "    similarities_t = similarities.T\n",
    "    recall_1_t = recall_at_k(similarities_t, k=1)\n",
    "    recall_5_t = recall_at_k(similarities_t, k=5)\n",
    "    recall_10_t = recall_at_k(similarities_t, k=10)\n",
    "    \n",
    "    print(f\"\\nText-to-Image Retrieval:\")\n",
    "    print(f\"Recall@1: {recall_1_t:.3f}\")\n",
    "    print(f\"Recall@5: {recall_5_t:.3f}\")\n",
    "    print(f\"Recall@10: {recall_10_t:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'i2t_recall_1': recall_1,\n",
    "        'i2t_recall_5': recall_5,\n",
    "        'i2t_recall_10': recall_10,\n",
    "        't2i_recall_1': recall_1_t,\n",
    "        't2i_recall_5': recall_5_t,\n",
    "        't2i_recall_10': recall_10_t\n",
    "    }\n",
    "\n",
    "# Example evaluation (replace with your test data)\n",
    "# test_images = [img1, img2, img3, ...]  # PIL Images\n",
    "# test_texts = [\"caption1\", \"caption2\", \"caption3\", ...]\n",
    "# metrics = evaluate_retrieval(model, test_images, test_texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}