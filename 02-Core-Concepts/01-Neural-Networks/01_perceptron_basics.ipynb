{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron: Building Block of Neural Networks\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the perceptron model and its components\n",
    "- Learn how perceptrons make binary decisions\n",
    "- Implement the perceptron learning algorithm\n",
    "- Understand linear separability and limitations\n",
    "- Visualize decision boundaries\n",
    "\n",
    "## What is a Perceptron?\n",
    "A perceptron is the simplest form of artificial neural network, consisting of a single neuron that makes binary decisions based on weighted inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perceptron Mathematical Model\n",
    "\n",
    "### Mathematical Formula:\n",
    "$$y = f(\\sum_{i=1}^{n} w_i x_i + b)$$\n",
    "\n",
    "Where:\n",
    "- $x_i$: Input features\n",
    "- $w_i$: Weights\n",
    "- $b$: Bias\n",
    "- $f$: Activation function (step function for perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.errors = []\n",
    "    \n",
    "    def step_function(self, x):\n",
    "        \"\"\"Step activation function\"\"\"\n",
    "        return np.where(x >= 0, 1, 0)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the perceptron\"\"\"\n",
    "        # Initialize weights and bias\n",
    "        n_features = X.shape[1]\n",
    "        self.weights = np.random.normal(0, 0.01, n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for iteration in range(self.max_iterations):\n",
    "            errors = 0\n",
    "            \n",
    "            for i in range(X.shape[0]):\n",
    "                # Forward pass\n",
    "                linear_output = np.dot(X[i], self.weights) + self.bias\n",
    "                prediction = self.step_function(linear_output)\n",
    "                \n",
    "                # Calculate error\n",
    "                error = y[i] - prediction\n",
    "                \n",
    "                # Update weights and bias if there's an error\n",
    "                if error != 0:\n",
    "                    self.weights += self.learning_rate * error * X[i]\n",
    "                    self.bias += self.learning_rate * error\n",
    "                    errors += 1\n",
    "            \n",
    "            self.errors.append(errors)\n",
    "            \n",
    "            # Stop if no errors (convergence)\n",
    "            if errors == 0:\n",
    "                print(f\"Converged after {iteration + 1} iterations\")\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        return self.step_function(linear_output)\n",
    "    \n",
    "    def decision_boundary(self, X):\n",
    "        \"\"\"Calculate decision boundary values\"\"\"\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "print(\"Perceptron class implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Example: AND Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND gate truth table\n",
    "X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_and = np.array([0, 0, 0, 1])\n",
    "\n",
    "print(\"AND Gate Truth Table:\")\n",
    "print(\"Input 1 | Input 2 | Output\")\n",
    "print(\"--------|---------|-------\")\n",
    "for i in range(len(X_and)):\n",
    "    print(f\"   {X_and[i][0]}    |    {X_and[i][1]}    |   {y_and[i]}\")\n",
    "\n",
    "# Train perceptron on AND gate\n",
    "perceptron_and = Perceptron(learning_rate=0.1, max_iterations=100)\n",
    "perceptron_and.fit(X_and, y_and)\n",
    "\n",
    "# Test predictions\n",
    "predictions = perceptron_and.predict(X_and)\n",
    "print(f\"\\nPredictions: {predictions}\")\n",
    "print(f\"Actual:      {y_and}\")\n",
    "print(f\"Accuracy: {np.mean(predictions == y_and) * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\nLearned weights: {perceptron_and.weights}\")\n",
    "print(f\"Learned bias: {perceptron_and.bias:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing the Learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(perceptron_and.errors, marker='o')\n",
    "plt.title('Perceptron Learning Curve (AND Gate)')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Number of Errors')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Visualize decision boundary\n",
    "def plot_decision_boundary(X, y, perceptron, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Create a mesh for plotting decision boundary\n",
    "    h = 0.01\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Make predictions on the mesh\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = perceptron.predict(mesh_points)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "    \n",
    "    # Plot data points\n",
    "    colors = ['red', 'blue']\n",
    "    for i in range(2):\n",
    "        plt.scatter(X[y == i, 0], X[y == i, 1], \n",
    "                   c=colors[i], marker='o', s=100, \n",
    "                   label=f'Class {i}', edgecolors='black')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Input 1')\n",
    "    plt.ylabel('Input 2')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(X_and, y_and, perceptron_and, 'Perceptron Decision Boundary (AND Gate)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. OR Gate Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR gate truth table\n",
    "X_or = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_or = np.array([0, 1, 1, 1])\n",
    "\n",
    "print(\"OR Gate Truth Table:\")\n",
    "print(\"Input 1 | Input 2 | Output\")\n",
    "print(\"--------|---------|-------\")\n",
    "for i in range(len(X_or)):\n",
    "    print(f\"   {X_or[i][0]}    |    {X_or[i][1]}    |   {y_or[i]}\")\n",
    "\n",
    "# Train perceptron on OR gate\n",
    "perceptron_or = Perceptron(learning_rate=0.1, max_iterations=100)\n",
    "perceptron_or.fit(X_or, y_or)\n",
    "\n",
    "# Test predictions\n",
    "predictions = perceptron_or.predict(X_or)\n",
    "print(f\"\\nPredictions: {predictions}\")\n",
    "print(f\"Actual:      {y_or}\")\n",
    "print(f\"Accuracy: {np.mean(predictions == y_or) * 100:.1f}%\")\n",
    "\n",
    "plot_decision_boundary(X_or, y_or, perceptron_or, 'Perceptron Decision Boundary (OR Gate)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The XOR Problem: Perceptron Limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR gate truth table\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])\n",
    "\n",
    "print(\"XOR Gate Truth Table:\")\n",
    "print(\"Input 1 | Input 2 | Output\")\n",
    "print(\"--------|---------|-------\")\n",
    "for i in range(len(X_xor)):\n",
    "    print(f\"   {X_xor[i][0]}    |    {X_xor[i][1]}    |   {y_xor[i]}\")\n",
    "\n",
    "# Try to train perceptron on XOR gate\n",
    "perceptron_xor = Perceptron(learning_rate=0.1, max_iterations=100)\n",
    "perceptron_xor.fit(X_xor, y_xor)\n",
    "\n",
    "# Test predictions\n",
    "predictions = perceptron_xor.predict(X_xor)\n",
    "print(f\"\\nPredictions: {predictions}\")\n",
    "print(f\"Actual:      {y_xor}\")\n",
    "print(f\"Accuracy: {np.mean(predictions == y_xor) * 100:.1f}%\")\n",
    "\n",
    "# Plot XOR problem\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['red', 'blue']\n",
    "for i in range(2):\n",
    "    plt.scatter(X_xor[y_xor == i, 0], X_xor[y_xor == i, 1], \n",
    "               c=colors[i], marker='o', s=100, \n",
    "               label=f'Class {i}', edgecolors='black')\n",
    "\n",
    "plt.title('XOR Problem: Not Linearly Separable')\n",
    "plt.xlabel('Input 1')\n",
    "plt.ylabel('Input 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.text(0.5, 0.8, 'No single line can\\nseparate these classes!', \n",
    "         ha='center', va='center', fontsize=12, \n",
    "         bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "plt.show()\n",
    "\n",
    "# Show learning curve for XOR\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(perceptron_xor.errors, marker='o', color='red')\n",
    "plt.title('Perceptron Learning Curve (XOR Gate) - No Convergence')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Number of Errors')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Real Dataset Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a linearly separable dataset\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, \n",
    "                  random_state=42, cluster_std=1.5)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train perceptron\n",
    "perceptron_real = Perceptron(learning_rate=0.01, max_iterations=1000)\n",
    "perceptron_real.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "train_predictions = perceptron_real.predict(X_train)\n",
    "test_predictions = perceptron_real.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = np.mean(train_predictions == y_train) * 100\n",
    "test_accuracy = np.mean(test_predictions == y_test) * 100\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.1f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.1f}%\")\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Training data\n",
    "colors = ['red', 'blue']\n",
    "for i in range(2):\n",
    "    axes[0].scatter(X_train[y_train == i, 0], X_train[y_train == i, 1], \n",
    "                   c=colors[i], marker='o', s=50, \n",
    "                   label=f'Class {i}', alpha=0.7)\n",
    "\n",
    "# Plot decision boundary for training data\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = perceptron_real.predict(mesh_points)\n",
    "Z = Z.reshape(xx.shape)\n",
    "axes[0].contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "axes[0].set_title(f'Training Data (Accuracy: {train_accuracy:.1f}%)')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test data\n",
    "for i in range(2):\n",
    "    axes[1].scatter(X_test[y_test == i, 0], X_test[y_test == i, 1], \n",
    "                   c=colors[i], marker='o', s=50, \n",
    "                   label=f'Class {i}', alpha=0.7)\n",
    "\n",
    "# Plot decision boundary for test data\n",
    "x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1\n",
    "y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = perceptron_real.predict(mesh_points)\n",
    "Z = Z.reshape(xx.shape)\n",
    "axes[1].contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "axes[1].set_title(f'Test Data (Accuracy: {test_accuracy:.1f}%)')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(perceptron_real.errors, marker='o')\n",
    "plt.title('Perceptron Learning Curve (Real Dataset)')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Number of Errors')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Perceptron vs Scikit-learn Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron as SKPerceptron\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train scikit-learn perceptron\n",
    "sk_perceptron = SKPerceptron(max_iter=1000, random_state=42)\n",
    "sk_perceptron.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "sk_train_pred = sk_perceptron.predict(X_train)\n",
    "sk_test_pred = sk_perceptron.predict(X_test)\n",
    "\n",
    "# Compare results\n",
    "print(\"Comparison: Our Implementation vs Scikit-learn\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Our Perceptron - Train Accuracy: {train_accuracy:.1f}%\")\n",
    "print(f\"Our Perceptron - Test Accuracy:  {test_accuracy:.1f}%\")\n",
    "print(f\"Scikit-learn - Train Accuracy:   {accuracy_score(y_train, sk_train_pred) * 100:.1f}%\")\n",
    "print(f\"Scikit-learn - Test Accuracy:    {accuracy_score(y_test, sk_test_pred) * 100:.1f}%\")\n",
    "\n",
    "print(\"\\nDetailed Classification Report (Scikit-learn):\")\n",
    "print(classification_report(y_test, sk_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Understanding Linear Separability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different types of datasets\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 1. Linearly separable data\n",
    "X1, y1 = make_blobs(n_samples=100, centers=2, n_features=2, \n",
    "                    random_state=42, cluster_std=1.0)\n",
    "axes[0, 0].scatter(X1[y1 == 0, 0], X1[y1 == 0, 1], c='red', alpha=0.7, label='Class 0')\n",
    "axes[0, 0].scatter(X1[y1 == 1, 0], X1[y1 == 1, 1], c='blue', alpha=0.7, label='Class 1')\n",
    "axes[0, 0].set_title('Linearly Separable\\n(Perceptron will work)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Overlapping classes\n",
    "X2, y2 = make_blobs(n_samples=100, centers=2, n_features=2, \n",
    "                    random_state=42, cluster_std=2.5)\n",
    "axes[0, 1].scatter(X2[y2 == 0, 0], X2[y2 == 0, 1], c='red', alpha=0.7, label='Class 0')\n",
    "axes[0, 1].scatter(X2[y2 == 1, 0], X2[y2 == 1, 1], c='blue', alpha=0.7, label='Class 1')\n",
    "axes[0, 1].set_title('Overlapping Classes\\n(Perceptron may struggle)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Circular pattern (not linearly separable)\n",
    "theta = np.linspace(0, 2*np.pi, 50)\n",
    "r1, r2 = 2, 4\n",
    "X3_inner = np.column_stack([r1 * np.cos(theta), r1 * np.sin(theta)])\n",
    "X3_outer = np.column_stack([r2 * np.cos(theta), r2 * np.sin(theta)])\n",
    "X3 = np.vstack([X3_inner, X3_outer])\n",
    "y3 = np.hstack([np.zeros(50), np.ones(50)])\n",
    "axes[1, 0].scatter(X3[y3 == 0, 0], X3[y3 == 0, 1], c='red', alpha=0.7, label='Class 0')\n",
    "axes[1, 0].scatter(X3[y3 == 1, 0], X3[y3 == 1, 1], c='blue', alpha=0.7, label='Class 1')\n",
    "axes[1, 0].set_title('Circular Pattern\\n(Not linearly separable)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. XOR pattern\n",
    "X4 = np.array([[0, 0], [0, 1], [1, 0], [1, 1], \n",
    "               [0.1, 0.1], [0.1, 0.9], [0.9, 0.1], [0.9, 0.9]])\n",
    "y4 = np.array([0, 1, 1, 0, 0, 1, 1, 0])\n",
    "axes[1, 1].scatter(X4[y4 == 0, 0], X4[y4 == 0, 1], c='red', alpha=0.7, s=100, label='Class 0')\n",
    "axes[1, 1].scatter(X4[y4 == 1, 0], X4[y4 == 1, 1], c='blue', alpha=0.7, s=100, label='Class 1')\n",
    "axes[1, 1].set_title('XOR Pattern\\n(Not linearly separable)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "1. **Perceptron Structure**: Single neuron with weights, bias, and step activation\n",
    "2. **Learning Algorithm**: Weight updates based on prediction errors\n",
    "3. **Linear Separability**: Perceptrons can only solve linearly separable problems\n",
    "4. **Limitations**: Cannot solve XOR and other non-linearly separable problems\n",
    "\n",
    "### Mathematical Insights:\n",
    "- **Decision Boundary**: $w_1x_1 + w_2x_2 + b = 0$\n",
    "- **Weight Update Rule**: $w_{new} = w_{old} + \\eta \\cdot error \\cdot input$\n",
    "- **Convergence**: Guaranteed for linearly separable data\n",
    "\n",
    "### Practical Applications:\n",
    "- Binary classification tasks\n",
    "- Linear decision boundaries\n",
    "- Foundation for multi-layer networks\n",
    "\n",
    "## üìù Exercises\n",
    "\n",
    "### Beginner:\n",
    "1. Implement NAND gate using perceptron\n",
    "2. Experiment with different learning rates\n",
    "3. Visualize weight changes during training\n",
    "\n",
    "### Intermediate:\n",
    "1. Add momentum to the learning algorithm\n",
    "2. Implement different activation functions\n",
    "3. Create a perceptron for multi-class classification\n",
    "\n",
    "### Advanced:\n",
    "1. Analyze convergence properties mathematically\n",
    "2. Implement pocket algorithm for non-separable data\n",
    "3. Compare with other linear classifiers\n",
    "\n",
    "## üîó Next Steps\n",
    "\n",
    "The perceptron's limitation with non-linearly separable data leads us to:\n",
    "1. **Multi-Layer Perceptrons (MLPs)**: Adding hidden layers\n",
    "2. **Non-linear Activation Functions**: Beyond step functions\n",
    "3. **Backpropagation**: Training multi-layer networks\n",
    "\n",
    "**Next Notebook**: [Multi-Layer Perceptrons](./02_multilayer_perceptron.ipynb) ‚Üí"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}